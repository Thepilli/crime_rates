{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c63d810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load types\n",
    "types_df = pd.read_csv(\n",
    "    \"C://Users//jirip//Documents//Developer//python//kriminalita//types.csv\",\n",
    "    sep=\",\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "# Build direct parent map\n",
    "parent_map = (\n",
    "    types_df\n",
    "    .melt(id_vars='id', value_vars=['parent_id1', 'parent_id2', 'parent_id3'], value_name='parent_id')\n",
    "    .dropna(subset=['parent_id'])\n",
    "    .query('parent_id > 0')\n",
    "    .groupby('id')['parent_id']\n",
    "    .apply(set)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Initialize ancestor map\n",
    "ancestor_map = {tid: set() for tid in types_df['id']}\n",
    "\n",
    "# Iteratively propagate ancestors\n",
    "updated = True\n",
    "while updated:\n",
    "    updated = False\n",
    "    for child, parents in parent_map.items():\n",
    "        current_ancestors = ancestor_map[child].copy()\n",
    "        for parent in parents:\n",
    "            current_ancestors.add(parent)\n",
    "            current_ancestors.update(ancestor_map[parent])  # inherit ancestors of parent\n",
    "        if current_ancestors != ancestor_map[child]:\n",
    "            ancestor_map[child] = current_ancestors\n",
    "            updated = True  # Keep looping if anything changed\n",
    "\n",
    "# Now ancestor_map holds: {type_id: set(all ancestors)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cd1303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# === 1. Load and Prepare Types Data ===\n",
    "types_df = pd.read_csv(\n",
    "    \"C://Users//jirip//Documents//Developer//python//kriminalita//types.csv\",\n",
    "    sep=\",\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "type_lookup = types_df.set_index('id')[['name', 'label']].to_dict('index')\n",
    "\n",
    "def expand_hierarchy(row):\n",
    "    ids = [row['parent_id1'], row['parent_id2'], row['parent_id3'], row['id']]\n",
    "    labels = [type_lookup.get(i, {'label': None})['label'] if i > 0 else None for i in ids]\n",
    "    names = [type_lookup.get(i, {'name': None})['name'] if i > 0 else None for i in ids]\n",
    "\n",
    "    result = {}\n",
    "    for level, (tid, label, name) in enumerate(zip(ids, labels, names), 1):\n",
    "        result[f'level_{level}_id'] = tid if tid > 0 else None\n",
    "        result[f'level_{level}_label'] = label\n",
    "        result[f'level_{level}_name'] = name\n",
    "    return pd.Series(result)\n",
    "\n",
    "dimension_df = types_df.apply(expand_hierarchy, axis=1)\n",
    "dimension_df = pd.concat([types_df[['id']], dimension_df, types_df[['name', 'label']]], axis=1)\n",
    "\n",
    "# Save dimension table\n",
    "dimension_df.to_csv('dim_type.csv', index=False)\n",
    "\n",
    "# === 2. Build Type Ancestor Map ===\n",
    "# Build direct parent map\n",
    "parent_map = (\n",
    "    types_df\n",
    "    .melt(id_vars='id', value_vars=['parent_id1', 'parent_id2', 'parent_id3'], value_name='parent_id')\n",
    "    .dropna(subset=['parent_id'])\n",
    "    .query('parent_id > 0')\n",
    "    .groupby('id')['parent_id']\n",
    "    .apply(set)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Initialize ancestor map\n",
    "ancestor_map = {tid: set() for tid in types_df['id']}\n",
    "\n",
    "# Iteratively propagate ancestors\n",
    "updated = True\n",
    "while updated:\n",
    "    updated = False\n",
    "    for child, parents in parent_map.items():\n",
    "        current_ancestors = ancestor_map[child].copy()\n",
    "        for parent in parents:\n",
    "            current_ancestors.add(parent)\n",
    "            current_ancestors.update(ancestor_map[parent])  # inherit ancestors of parent\n",
    "        if current_ancestors != ancestor_map[child]:\n",
    "            ancestor_map[child] = current_ancestors\n",
    "            updated = True  # Keep looping if anything changed\n",
    "\n",
    "# Now ancestor_map holds: {type_id: set(all ancestors)}\n",
    "\n",
    "\n",
    "type_ancestors = ancestor_map\n",
    "\n",
    "# === 3. Load Fact Data ===\n",
    "fact_df = pd.read_csv(\n",
    "    \"C://Users//jirip//Documents//Developer//python//kriminalita//202504.csv\",\n",
    "    sep=\",\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "# === 4. Filter Most Granular Types ===\n",
    "def filter_most_granular(group):\n",
    "    type_ids = set(group['types'])\n",
    "\n",
    "    # Remove non-granular types\n",
    "    to_remove = set()\n",
    "    for t in type_ids:\n",
    "        ancestors = type_ancestors.get(t, set())\n",
    "        to_remove.update(type_ids & ancestors)  # Only care if ancestors exist in current types\n",
    "\n",
    "    granular_types = type_ids - to_remove\n",
    "\n",
    "    # Take common attributes once\n",
    "    row = group.iloc[0]\n",
    "    common_data = {\n",
    "        'id': row['id'],\n",
    "        'Longitude': row['x'],\n",
    "        'Latitude': row['y'],\n",
    "        'mp': row['mp'],\n",
    "        'date': row['date'],\n",
    "        'state': row['state'],\n",
    "        'relevance': row['relevance'],\n",
    "    }\n",
    "\n",
    "    # Expand into rows\n",
    "    records = [\n",
    "        {**common_data, 'types': t}\n",
    "        for t in granular_types\n",
    "    ]\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "# === 5. Process and Save ===\n",
    "filtered_fact_df = (\n",
    "    fact_df\n",
    "    .groupby('id', group_keys=False)\n",
    "    .apply(filter_most_granular)\n",
    "    .reset_index(drop=True)\n",
    "    .sort_values(by=['id', 'types'])\n",
    "    .assign(is_one=lambda df: df.groupby('id').cumcount() + 1)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "filtered_fact_df.to_csv('fact_clean.csv', index=False)\n",
    "\n",
    "# Preview the cleaned fact table\n",
    "print(filtered_fact_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756cfe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# === 1. Load and Prepare Types Data ===\n",
    "types_df = pd.read_csv(\n",
    "    \"C://Users//jirip//Documents//Developer//python//kriminalita//types.csv\",\n",
    "    sep=\",\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "type_lookup = types_df.set_index('id')[['name', 'label']].to_dict('index')\n",
    "\n",
    "def expand_hierarchy(row):\n",
    "    ids = [row['parent_id1'], row['parent_id2'], row['parent_id3'], row['id']]\n",
    "    labels = [type_lookup.get(i, {'label': None})['label'] if i > 0 else None for i in ids]\n",
    "    names = [type_lookup.get(i, {'name': None})['name'] if i > 0 else None for i in ids]\n",
    "\n",
    "    result = {}\n",
    "    for level, (tid, label, name) in enumerate(zip(ids, labels, names), 1):\n",
    "        result[f'level_{level}_id'] = tid if tid > 0 else None\n",
    "        result[f'level_{level}_label'] = label\n",
    "        result[f'level_{level}_name'] = name\n",
    "    return pd.Series(result)\n",
    "\n",
    "dimension_df = types_df.apply(expand_hierarchy, axis=1)\n",
    "dimension_df = pd.concat([types_df[['id']], dimension_df, types_df[['name', 'label']]], axis=1)\n",
    "\n",
    "# Save dimension table\n",
    "dimension_df.to_csv('dim_type_new.csv', index=False)\n",
    "\n",
    "# === 2. Build Type Ancestor Map ===\n",
    "def build_type_ancestor_map(df: pd.DataFrame) -> dict[int, set[int]]:\n",
    "    def collect_ancestors(type_id: int) -> set[int]:\n",
    "        if not type_id or pd.isna(type_id):\n",
    "            return set()\n",
    "\n",
    "        row = df.loc[df['id'] == type_id]\n",
    "        if row.empty:\n",
    "            return set()\n",
    "\n",
    "        direct_parents = {pid for pid in row[['parent_id1', 'parent_id2', 'parent_id3']].values.flatten() if pid > 0}\n",
    "\n",
    "        ancestors = set(direct_parents)\n",
    "        for parent in direct_parents:\n",
    "            ancestors |= collect_ancestors(parent)\n",
    "\n",
    "        return ancestors\n",
    "\n",
    "    return {tid: collect_ancestors(tid) for tid in df['id']}\n",
    "\n",
    "type_ancestors = build_type_ancestor_map(types_df)\n",
    "\n",
    "# === 3. Load Fact Data ===\n",
    "fact_df = pd.read_csv(\n",
    "    \"C://Users//jirip//Documents//Developer//python//kriminalita//202504.csv\",\n",
    "    sep=\",\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "# === 4. Filter Most Granular Types ===\n",
    "def filter_most_granular(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    type_ids = set(group['types'])\n",
    "    to_remove = {\n",
    "        t1 for t1 in type_ids\n",
    "        for t2 in type_ids\n",
    "        if t1 != t2 and t1 in type_ancestors.get(t2, set())\n",
    "    }\n",
    "    granular_types = type_ids - to_remove\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'id': [group['id'].iloc[0]] * len(granular_types),\n",
    "        'types': list(granular_types),\n",
    "        'Longitude': group['x'].iloc[0],\n",
    "        'Latitude': group['y'].iloc[0],\n",
    "        'mp': group['mp'].iloc[0],\n",
    "        'date': group['date'].iloc[0],\n",
    "        'state': group['state'].iloc[0],\n",
    "        'relevance': group['relevance'].iloc[0]\n",
    "    })\n",
    "\n",
    "# === 5. Process and Save ===\n",
    "filtered_fact_df = (\n",
    "    fact_df\n",
    "    .groupby('id', group_keys=False)\n",
    "    .apply(filter_most_granular)\n",
    "    .reset_index(drop=True)\n",
    "    .sort_values(by=['id', 'types'])\n",
    "    .assign(is_one=lambda df: df.groupby('id').cumcount() + 1)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "filtered_fact_df.to_csv('fact_clean_new.csv', index=False)\n",
    "\n",
    "# Preview the cleaned fact table\n",
    "print(filtered_fact_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e469237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_types(types_path):\n",
    "    types_df = pd.read_csv(types_path, sep=\",\", encoding=\"utf-8\")\n",
    "    return types_df\n",
    "\n",
    "def build_parent_map(types_df):\n",
    "    parent_map = defaultdict(set)\n",
    "    for _, row in types_df.iterrows():\n",
    "        tid = row['id']\n",
    "        for pid in (row['parent_id1'], row['parent_id2'], row['parent_id3']):\n",
    "            if pid > 0:\n",
    "                parent_map[tid].add(pid)\n",
    "    return parent_map\n",
    "\n",
    "def build_ancestor_map(parent_map):\n",
    "    ancestor_map = defaultdict(set)\n",
    "\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        for child, parents in parent_map.items():\n",
    "            current_ancestors = ancestor_map[child]\n",
    "            new_ancestors = set()\n",
    "            for parent in parents:\n",
    "                new_ancestors.add(parent)\n",
    "                new_ancestors.update(ancestor_map[parent])\n",
    "            if not new_ancestors.issubset(current_ancestors):\n",
    "                ancestor_map[child].update(new_ancestors)\n",
    "                changed = True\n",
    "    return dict(ancestor_map)\n",
    "\n",
    "def load_fact_table(fact_path):\n",
    "    fact_df = pd.read_csv(fact_path, sep=\",\", encoding=\"utf-8\")\n",
    "    return fact_df\n",
    "\n",
    "def filter_most_granular(group, type_ancestors):\n",
    "    type_ids = set(group['types'])\n",
    "\n",
    "    # Remove types that are ancestors of others\n",
    "    to_remove = set()\n",
    "    for t in type_ids:\n",
    "        ancestors = type_ancestors.get(t, set())\n",
    "        to_remove.update(type_ids & ancestors)\n",
    "\n",
    "    granular_types = type_ids - to_remove\n",
    "\n",
    "    # Fixed attributes\n",
    "    row = group.iloc[0]\n",
    "    common_data = {\n",
    "        'id': row['id'],\n",
    "        'Longitude': row['x'],\n",
    "        'Latitude': row['y'],\n",
    "        'mp': row['mp'],\n",
    "        'date': row['date'],\n",
    "        'state': row['state'],\n",
    "        'relevance': row['relevance'],\n",
    "    }\n",
    "\n",
    "    # Expand\n",
    "    records = [\n",
    "        {**common_data, 'types': t}\n",
    "        for t in granular_types\n",
    "    ]\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# def clean_fact_table(fact_df, type_ancestors):\n",
    "#     # Group and apply filtering\n",
    "#     filtered_df = fact_df.groupby('id', group_keys=False).apply(\n",
    "#         lambda group: filter_most_granular(group, type_ancestors)\n",
    "#     ).reset_index(drop=True)\n",
    "# \n",
    "#     # Assign 'is_one'\n",
    "#     filtered_df['is_one'] = (\n",
    "#         filtered_df.sort_values(by=['id', 'types'])\n",
    "#         .groupby('id')\n",
    "#         .cumcount() + 1\n",
    "#     )\n",
    "# \n",
    "#     return filtered_df.sort_values(by=['id', 'types']).reset_index(drop=True)\n",
    "\n",
    "def clean_fact_table(fact_df, type_ancestors):\n",
    "    # Apply filtering per group while avoiding the deprecation warning\n",
    "    grouped = fact_df.groupby('id', group_keys=False)\n",
    "\n",
    "    # Explicitly remove grouping columns from the group if not needed in function\n",
    "    filtered_df = grouped.apply(\n",
    "        lambda group: filter_most_granular(group.copy(), type_ancestors),\n",
    "        include_group=False  # Avoid future deprecation\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # Assign 'is_one'\n",
    "    filtered_df['is_one'] = (\n",
    "        filtered_df.sort_values(by=['id', 'types'])\n",
    "        .groupby('id')\n",
    "        .cumcount() + 1\n",
    "    )\n",
    "\n",
    "    return filtered_df.sort_values(by=['id', 'types']).reset_index(drop=True)\n",
    "\n",
    "def save_fact_table(filtered_df, output_path):\n",
    "    filtered_df.to_csv(output_path, index=False)\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Paths\n",
    "    types_path = \"C://Users//jirip//Documents//Developer//python//kriminalita//types.csv\"\n",
    "    \n",
    "    yearMonts = [\"202412\",\"202411\",\"202410\",\"202409\",\"202408\",\"202407\",\"202406\",\"202405\",\"202404\",\"202403\",\"202402\",\"202401\"]\n",
    "    for yearMonth in yearMonts:\n",
    "        loop_start_time = time.time()\n",
    "        fact_path = f\"C://Users//jirip//Documents//Developer//python//kriminalita//source_files//{yearMonth}.csv\"\n",
    "        output_path = f\"fact_clean_{yearMonth}.csv\"\n",
    "        print(f\"Processing {yearMonth}...\")\n",
    "        # Load\n",
    "        types_df = load_types(types_path)\n",
    "        fact_df = load_fact_table(fact_path)\n",
    "\n",
    "        # Build maps\n",
    "        parent_map = build_parent_map(types_df)\n",
    "        type_ancestors = build_ancestor_map(parent_map)\n",
    "\n",
    "        # Clean fact table\n",
    "        cleaned_fact_df = clean_fact_table(fact_df, type_ancestors)\n",
    "\n",
    "        # Save\n",
    "        save_fact_table(cleaned_fact_df, output_path)\n",
    "        \n",
    "        elapsed = time.time() - loop_start_time\n",
    "        print(f\"Processed {yearMonth} in {elapsed:.2f} seconds.\")    \n",
    "\n",
    "\n",
    "    \n",
    "    # Print elapsed time\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Completed in {elapsed:.2f} seconds.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c415205d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 202501...\n",
      "Len of 202501... 338707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jirip\\AppData\\Local\\Temp\\ipykernel_17536\\1537796656.py:83: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "  (pl.arange(0, pl.count()).over(\"id\")).alias(\"is_one\") + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of cleaned 202501... 125717\n",
      "Processed 202501 in 0.59 seconds.\n",
      "Processing 202502...\n",
      "Len of 202502... 324007\n",
      "Len of cleaned 202502... 119628\n",
      "Processed 202502 in 0.52 seconds.\n",
      "Processing 202503...\n",
      "Len of 202503... 357619\n",
      "Len of cleaned 202503... 131215\n",
      "Processed 202503 in 0.68 seconds.\n",
      "Processing 202504...\n",
      "Len of 202504... 310501\n",
      "Len of cleaned 202504... 111895\n",
      "Processed 202504 in 0.48 seconds.\n",
      "Processing 202401...\n",
      "Len of 202401... 336512\n",
      "Len of cleaned 202401... 126193\n",
      "Processed 202401 in 0.55 seconds.\n",
      "Processing 202402...\n",
      "Len of 202402... 358757\n",
      "Len of cleaned 202402... 132984\n",
      "Processed 202402 in 0.64 seconds.\n",
      "Processing 202403...\n",
      "Len of 202403... 372085\n",
      "Len of cleaned 202403... 137538\n",
      "Processed 202403 in 0.64 seconds.\n",
      "Processing 202404...\n",
      "Len of 202404... 373364\n",
      "Len of cleaned 202404... 138152\n",
      "Processed 202404 in 0.63 seconds.\n",
      "Processing 202405...\n",
      "Len of 202405... 372166\n",
      "Len of cleaned 202405... 138293\n",
      "Processed 202405 in 0.66 seconds.\n",
      "Processing 202406...\n",
      "Len of 202406... 332026\n",
      "Len of cleaned 202406... 124248\n",
      "Processed 202406 in 0.55 seconds.\n",
      "Processing 202407...\n",
      "Len of 202407... 358676\n",
      "Len of cleaned 202407... 133469\n",
      "Processed 202407 in 0.61 seconds.\n",
      "Processing 202408...\n",
      "Len of 202408... 349134\n",
      "Len of cleaned 202408... 130011\n",
      "Processed 202408 in 0.68 seconds.\n",
      "Processing 202409...\n",
      "Len of 202409... 309109\n",
      "Len of cleaned 202409... 116377\n",
      "Processed 202409 in 0.56 seconds.\n",
      "Processing 202410...\n",
      "Len of 202410... 350853\n",
      "Len of cleaned 202410... 130803\n",
      "Processed 202410 in 0.62 seconds.\n",
      "Processing 202411...\n",
      "Len of 202411... 268808\n",
      "Len of cleaned 202411... 102708\n",
      "Processed 202411 in 0.47 seconds.\n",
      "Processing 202412...\n",
      "Len of 202412... 235469\n",
      "Len of cleaned 202412... 90484\n",
      "Processed 202412 in 0.43 seconds.\n",
      "Processing 202301...\n",
      "Len of 202301... 370824\n",
      "Len of cleaned 202301... 139069\n",
      "Processed 202301 in 0.66 seconds.\n",
      "Processing 202302...\n",
      "Len of 202302... 373365\n",
      "Len of cleaned 202302... 138557\n",
      "Processed 202302 in 0.73 seconds.\n",
      "Processing 202303...\n",
      "Len of 202303... 431959\n",
      "Len of cleaned 202303... 159677\n",
      "Processed 202303 in 0.98 seconds.\n",
      "Processing 202304...\n",
      "Len of 202304... 386610\n",
      "Len of cleaned 202304... 143564\n",
      "Processed 202304 in 0.98 seconds.\n",
      "Processing 202305...\n",
      "Len of 202305... 408159\n",
      "Len of cleaned 202305... 152030\n",
      "Processed 202305 in 0.94 seconds.\n",
      "Processing 202306...\n",
      "Len of 202306... 401396\n",
      "Len of cleaned 202306... 149108\n",
      "Processed 202306 in 0.92 seconds.\n",
      "Processing 202307...\n",
      "Len of 202307... 370674\n",
      "Len of cleaned 202307... 138165\n",
      "Processed 202307 in 0.76 seconds.\n",
      "Processing 202308...\n",
      "Len of 202308... 396479\n",
      "Len of cleaned 202308... 147977\n",
      "Processed 202308 in 0.89 seconds.\n",
      "Processing 202309...\n",
      "Len of 202309... 360511\n",
      "Len of cleaned 202309... 135170\n",
      "Processed 202309 in 0.74 seconds.\n",
      "Processing 202310...\n",
      "Len of 202310... 355422\n",
      "Len of cleaned 202310... 133840\n",
      "Processed 202310 in 0.75 seconds.\n",
      "Processing 202311...\n",
      "Len of 202311... 322348\n",
      "Len of cleaned 202311... 121933\n",
      "Processed 202311 in 0.70 seconds.\n",
      "Processing 202312...\n",
      "Len of 202312... 233339\n",
      "Len of cleaned 202312... 91034\n",
      "Processed 202312 in 0.54 seconds.\n",
      "Processing 202201...\n",
      "Len of 202201... 330929\n",
      "Len of cleaned 202201... 123527\n",
      "Processed 202201 in 0.82 seconds.\n",
      "Processing 202202...\n",
      "Len of 202202... 337669\n",
      "Len of cleaned 202202... 124910\n",
      "Processed 202202 in 0.89 seconds.\n",
      "Processing 202203...\n",
      "Len of 202203... 379982\n",
      "Len of cleaned 202203... 140471\n",
      "Processed 202203 in 1.11 seconds.\n",
      "Processing 202204...\n",
      "Len of 202204... 375937\n",
      "Len of cleaned 202204... 138965\n",
      "Processed 202204 in 1.18 seconds.\n",
      "Processing 202205...\n",
      "Len of 202205... 387982\n",
      "Len of cleaned 202205... 143860\n",
      "Processed 202205 in 1.16 seconds.\n",
      "Processing 202206...\n",
      "Len of 202206... 375515\n",
      "Len of cleaned 202206... 139739\n",
      "Processed 202206 in 0.94 seconds.\n",
      "Processing 202207...\n",
      "Len of 202207... 345474\n",
      "Len of cleaned 202207... 129287\n",
      "Processed 202207 in 0.73 seconds.\n",
      "Processing 202208...\n",
      "Len of 202208... 370643\n",
      "Len of cleaned 202208... 138896\n",
      "Processed 202208 in 0.71 seconds.\n",
      "Processing 202209...\n",
      "Len of 202209... 347587\n",
      "Len of cleaned 202209... 130559\n",
      "Processed 202209 in 0.79 seconds.\n",
      "Processing 202210...\n",
      "Len of 202210... 334193\n",
      "Len of cleaned 202210... 126855\n",
      "Processed 202210 in 0.74 seconds.\n",
      "Processing 202211...\n",
      "Len of 202211... 325358\n",
      "Len of cleaned 202211... 123080\n",
      "Processed 202211 in 0.86 seconds.\n",
      "Processing 202212...\n",
      "Len of 202212... 244774\n",
      "Len of cleaned 202212... 95506\n",
      "Processed 202212 in 0.65 seconds.\n",
      "Processing 202101...\n",
      "Len of 202101... 262335\n",
      "Len of cleaned 202101... 97786\n",
      "Processed 202101 in 0.65 seconds.\n",
      "Processing 202102...\n",
      "Len of 202102... 260697\n",
      "Len of cleaned 202102... 96291\n",
      "Processed 202102 in 0.70 seconds.\n",
      "Processing 202103...\n",
      "Len of 202103... 278619\n",
      "Len of cleaned 202103... 102660\n",
      "Processed 202103 in 0.61 seconds.\n",
      "Processing 202104...\n",
      "Len of 202104... 316556\n",
      "Len of cleaned 202104... 116075\n",
      "Processed 202104 in 0.80 seconds.\n",
      "Processing 202105...\n",
      "Len of 202105... 324131\n",
      "Len of cleaned 202105... 120241\n",
      "Processed 202105 in 0.78 seconds.\n",
      "Processing 202106...\n",
      "Len of 202106... 334272\n",
      "Len of cleaned 202106... 124322\n",
      "Processed 202106 in 0.71 seconds.\n",
      "Processing 202107...\n",
      "Len of 202107... 314290\n",
      "Len of cleaned 202107... 117692\n",
      "Processed 202107 in 0.69 seconds.\n",
      "Processing 202108...\n",
      "Len of 202108... 318476\n",
      "Len of cleaned 202108... 119503\n",
      "Processed 202108 in 0.67 seconds.\n",
      "Processing 202109...\n",
      "Len of 202109... 316145\n",
      "Len of cleaned 202109... 118542\n",
      "Processed 202109 in 0.63 seconds.\n",
      "Processing 202110...\n",
      "Len of 202110... 314690\n",
      "Len of cleaned 202110... 117895\n",
      "Processed 202110 in 0.64 seconds.\n",
      "Processing 202111...\n",
      "Len of 202111... 302282\n",
      "Len of cleaned 202111... 113477\n",
      "Processed 202111 in 0.64 seconds.\n",
      "Processing 202112...\n",
      "Len of 202112... 265943\n",
      "Len of cleaned 202112... 100676\n",
      "Processed 202112 in 0.63 seconds.\n",
      "Processing 202001...\n",
      "Len of 202001... 367182\n",
      "Len of cleaned 202001... 136626\n",
      "Processed 202001 in 0.88 seconds.\n",
      "Processing 202002...\n",
      "Len of 202002... 357909\n",
      "Len of cleaned 202002... 132586\n",
      "Processed 202002 in 0.92 seconds.\n",
      "Processing 202003...\n",
      "Len of 202003... 220089\n",
      "Len of cleaned 202003... 84522\n",
      "Processed 202003 in 1.05 seconds.\n",
      "Processing 202004...\n",
      "Len of 202004... 223533\n",
      "Len of cleaned 202004... 86218\n",
      "Processed 202004 in 0.67 seconds.\n",
      "Processing 202005...\n",
      "Len of 202005... 283423\n",
      "Len of cleaned 202005... 106669\n",
      "Processed 202005 in 0.63 seconds.\n",
      "Processing 202006...\n",
      "Len of 202006... 346064\n",
      "Len of cleaned 202006... 129099\n",
      "Processed 202006 in 1.98 seconds.\n",
      "Processing 202007...\n",
      "Len of 202007... 349552\n",
      "Len of cleaned 202007... 130327\n",
      "Processed 202007 in 1.48 seconds.\n",
      "Processing 202008...\n",
      "Len of 202008... 318052\n",
      "Len of cleaned 202008... 119520\n",
      "Processed 202008 in 1.04 seconds.\n",
      "Processing 202009...\n",
      "Len of 202009... 302887\n",
      "Len of cleaned 202009... 113742\n",
      "Processed 202009 in 0.67 seconds.\n",
      "Processing 202010...\n",
      "Len of 202010... 280944\n",
      "Len of cleaned 202010... 105182\n",
      "Processed 202010 in 0.58 seconds.\n",
      "Processing 202011...\n",
      "Len of 202011... 292626\n",
      "Len of cleaned 202011... 107701\n",
      "Processed 202011 in 0.61 seconds.\n",
      "Processing 202012...\n",
      "Len of 202012... 231162\n",
      "Len of cleaned 202012... 87605\n",
      "Processed 202012 in 0.62 seconds.\n",
      "Processing 201901...\n",
      "Len of 201901... 366405\n",
      "Len of cleaned 201901... 136748\n",
      "Processed 201901 in 0.74 seconds.\n",
      "Processing 201902...\n",
      "Len of 201902... 380250\n",
      "Len of cleaned 201902... 140322\n",
      "Processed 201902 in 0.71 seconds.\n",
      "Processing 201903...\n",
      "Len of 201903... 401350\n",
      "Len of cleaned 201903... 148741\n",
      "Processed 201903 in 0.79 seconds.\n",
      "Processing 201904...\n",
      "Len of 201904... 408705\n",
      "Len of cleaned 201904... 151048\n",
      "Processed 201904 in 0.80 seconds.\n",
      "Processing 201905...\n",
      "Len of 201905... 379592\n",
      "Len of cleaned 201905... 141709\n",
      "Processed 201905 in 0.72 seconds.\n",
      "Processing 201906...\n",
      "Len of 201906... 351396\n",
      "Len of cleaned 201906... 131926\n",
      "Processed 201906 in 0.72 seconds.\n",
      "Processing 201907...\n",
      "Len of 201907... 371322\n",
      "Len of cleaned 201907... 138504\n",
      "Processed 201907 in 0.68 seconds.\n",
      "Processing 201908...\n",
      "Len of 201908... 368639\n",
      "Len of cleaned 201908... 137563\n",
      "Processed 201908 in 0.82 seconds.\n",
      "Processing 201909...\n",
      "Len of 201909... 338145\n",
      "Len of cleaned 201909... 126830\n",
      "Processed 201909 in 0.65 seconds.\n",
      "Processing 201910...\n",
      "Len of 201910... 365151\n",
      "Len of cleaned 201910... 136661\n",
      "Processed 201910 in 0.75 seconds.\n",
      "Processing 201911...\n",
      "Len of 201911... 328356\n",
      "Len of cleaned 201911... 123842\n",
      "Processed 201911 in 0.66 seconds.\n",
      "Processing 201912...\n",
      "Len of 201912... 249449\n",
      "Len of cleaned 201912... 96435\n",
      "Processed 201912 in 0.58 seconds.\n",
      "Processing 201801...\n",
      "Len of 201801... 400388\n",
      "Len of cleaned 201801... 147681\n",
      "Processed 201801 in 0.95 seconds.\n",
      "Processing 201802...\n",
      "Len of 201802... 373215\n",
      "Len of cleaned 201802... 136857\n",
      "Processed 201802 in 0.75 seconds.\n",
      "Processing 201803...\n",
      "Len of 201803... 415983\n",
      "Len of cleaned 201803... 152424\n",
      "Processed 201803 in 0.99 seconds.\n",
      "Processing 201804...\n",
      "Len of 201804... 398339\n",
      "Len of cleaned 201804... 147362\n",
      "Processed 201804 in 0.68 seconds.\n",
      "Processing 201805...\n",
      "Len of 201805... 396207\n",
      "Len of cleaned 201805... 147386\n",
      "Processed 201805 in 0.70 seconds.\n",
      "Processing 201806...\n",
      "Len of 201806... 376938\n",
      "Len of cleaned 201806... 140456\n",
      "Processed 201806 in 0.63 seconds.\n",
      "Processing 201807...\n",
      "Len of 201807... 367341\n",
      "Len of cleaned 201807... 137053\n",
      "Processed 201807 in 0.63 seconds.\n",
      "Processing 201808...\n",
      "Len of 201808... 374330\n",
      "Len of cleaned 201808... 139725\n",
      "Processed 201808 in 0.87 seconds.\n",
      "Processing 201809...\n",
      "Len of 201809... 338277\n",
      "Len of cleaned 201809... 126787\n",
      "Processed 201809 in 0.56 seconds.\n",
      "Processing 201810...\n",
      "Len of 201810... 390695\n",
      "Len of cleaned 201810... 145470\n",
      "Processed 201810 in 0.61 seconds.\n",
      "Processing 201811...\n",
      "Len of 201811... 358429\n",
      "Len of cleaned 201811... 133732\n",
      "Processed 201811 in 0.56 seconds.\n",
      "Processing 201812...\n",
      "Len of 201812... 240915\n",
      "Len of cleaned 201812... 92876\n",
      "Processed 201812 in 0.38 seconds.\n",
      "Processing 201701...\n",
      "Len of 201701... 345945\n",
      "Len of cleaned 201701... 127511\n",
      "Processed 201701 in 0.58 seconds.\n",
      "Processing 201702...\n",
      "Len of 201702... 375633\n",
      "Len of cleaned 201702... 137960\n",
      "Processed 201702 in 0.75 seconds.\n",
      "Processing 201703...\n",
      "Len of 201703... 456075\n",
      "Len of cleaned 201703... 167311\n",
      "Processed 201703 in 1.10 seconds.\n",
      "Processing 201704...\n",
      "Len of 201704... 392751\n",
      "Len of cleaned 201704... 144955\n",
      "Processed 201704 in 1.24 seconds.\n",
      "Processing 201705...\n",
      "Len of 201705... 417617\n",
      "Len of cleaned 201705... 154389\n",
      "Processed 201705 in 0.82 seconds.\n",
      "Processing 201706...\n",
      "Len of 201706... 383879\n",
      "Len of cleaned 201706... 142911\n",
      "Processed 201706 in 0.77 seconds.\n",
      "Processing 201707...\n",
      "Len of 201707... 354403\n",
      "Len of cleaned 201707... 132346\n",
      "Processed 201707 in 0.67 seconds.\n",
      "Processing 201708...\n",
      "Len of 201708... 388565\n",
      "Len of cleaned 201708... 145123\n",
      "Processed 201708 in 0.73 seconds.\n",
      "Processing 201709...\n",
      "Len of 201709... 345463\n",
      "Len of cleaned 201709... 129408\n",
      "Processed 201709 in 0.66 seconds.\n",
      "Processing 201710...\n",
      "Len of 201710... 372312\n",
      "Len of cleaned 201710... 139483\n",
      "Processed 201710 in 0.67 seconds.\n",
      "Processing 201711...\n",
      "Len of 201711... 356832\n",
      "Len of cleaned 201711... 133323\n",
      "Processed 201711 in 0.63 seconds.\n",
      "Processing 201712...\n",
      "Len of 201712... 255716\n",
      "Len of cleaned 201712... 98269\n",
      "Processed 201712 in 0.49 seconds.\n",
      "Processing 201601...\n",
      "Len of 201601... 360034\n",
      "Len of cleaned 201601... 133982\n",
      "Processed 201601 in 1.27 seconds.\n",
      "Processing 201602...\n",
      "Len of 201602... 425699\n",
      "Len of cleaned 201602... 156364\n",
      "Processed 201602 in 0.86 seconds.\n",
      "Processing 201603...\n",
      "Len of 201603... 481273\n",
      "Len of cleaned 201603... 175868\n",
      "Processed 201603 in 1.09 seconds.\n",
      "Processing 201604...\n",
      "Len of 201604... 423301\n",
      "Len of cleaned 201604... 156476\n",
      "Processed 201604 in 0.92 seconds.\n",
      "Processing 201605...\n",
      "Len of 201605... 431584\n",
      "Len of cleaned 201605... 159812\n",
      "Processed 201605 in 1.02 seconds.\n",
      "Processing 201606...\n",
      "Len of 201606... 413420\n",
      "Len of cleaned 201606... 153293\n",
      "Processed 201606 in 0.97 seconds.\n",
      "Processing 201607...\n",
      "Len of 201607... 395896\n",
      "Len of cleaned 201607... 147120\n",
      "Processed 201607 in 0.85 seconds.\n",
      "Processing 201608...\n",
      "Len of 201608... 485267\n",
      "Len of cleaned 201608... 177727\n",
      "Processed 201608 in 1.34 seconds.\n",
      "Processing 201609...\n",
      "Len of 201609... 419137\n",
      "Len of cleaned 201609... 154877\n",
      "Processed 201609 in 1.10 seconds.\n",
      "Processing 201610...\n",
      "Len of 201610... 370660\n",
      "Len of cleaned 201610... 138400\n",
      "Processed 201610 in 0.86 seconds.\n",
      "Processing 201611...\n",
      "Len of 201611... 363969\n",
      "Len of cleaned 201611... 135420\n",
      "Processed 201611 in 0.90 seconds.\n",
      "Processing 201612...\n",
      "Len of 201612... 274388\n",
      "Len of cleaned 201612... 104422\n",
      "Processed 201612 in 0.74 seconds.\n",
      "Processing 201501...\n",
      "Len of 201501... 365795\n",
      "Len of cleaned 201501... 129193\n",
      "Processed 201501 in 0.81 seconds.\n",
      "Processing 201502...\n",
      "Len of 201502... 375689\n",
      "Len of cleaned 201502... 132108\n",
      "Processed 201502 in 0.76 seconds.\n",
      "Processing 201503...\n",
      "Len of 201503... 447279\n",
      "Len of cleaned 201503... 156875\n",
      "Processed 201503 in 0.99 seconds.\n",
      "Processing 201504...\n",
      "Len of 201504... 445507\n",
      "Len of cleaned 201504... 156462\n",
      "Processed 201504 in 0.87 seconds.\n",
      "Processing 201505...\n",
      "Len of 201505... 419703\n",
      "Len of cleaned 201505... 147830\n",
      "Processed 201505 in 0.97 seconds.\n",
      "Processing 201506...\n",
      "Len of 201506... 399427\n",
      "Len of cleaned 201506... 141346\n",
      "Processed 201506 in 0.76 seconds.\n",
      "Processing 201507...\n",
      "Len of 201507... 395299\n",
      "Len of cleaned 201507... 140157\n",
      "Processed 201507 in 0.73 seconds.\n",
      "Processing 201508...\n",
      "Len of 201508... 342350\n",
      "Len of cleaned 201508... 122463\n",
      "Processed 201508 in 0.66 seconds.\n",
      "Processing 201509...\n",
      "Len of 201509... 367616\n",
      "Len of cleaned 201509... 131272\n",
      "Processed 201509 in 0.69 seconds.\n",
      "Processing 201510...\n",
      "Len of 201510... 359151\n",
      "Len of cleaned 201510... 129091\n",
      "Processed 201510 in 0.62 seconds.\n",
      "Processing 201511...\n",
      "Len of 201511... 342166\n",
      "Len of cleaned 201511... 123450\n",
      "Processed 201511 in 0.63 seconds.\n",
      "Processing 201512...\n",
      "Len of 201512... 284164\n",
      "Len of cleaned 201512... 105570\n",
      "Processed 201512 in 0.61 seconds.\n",
      "Processing 201401...\n",
      "Len of 201401... 157327\n",
      "Len of cleaned 201401... 57910\n",
      "Processed 201401 in 0.37 seconds.\n",
      "Processing 201402...\n",
      "Len of 201402... 150748\n",
      "Len of cleaned 201402... 55518\n",
      "Processed 201402 in 0.33 seconds.\n",
      "Processing 201403...\n",
      "Len of 201403... 172783\n",
      "Len of cleaned 201403... 63454\n",
      "Processed 201403 in 0.40 seconds.\n",
      "Processing 201404...\n",
      "Len of 201404... 197321\n",
      "Len of cleaned 201404... 71369\n",
      "Processed 201404 in 0.63 seconds.\n",
      "Processing 201405...\n",
      "Len of 201405... 194806\n",
      "Len of cleaned 201405... 70553\n",
      "Processed 201405 in 0.54 seconds.\n",
      "Processing 201406...\n",
      "Len of 201406... 199774\n",
      "Len of cleaned 201406... 72142\n",
      "Processed 201406 in 0.58 seconds.\n",
      "Processing 201407...\n",
      "Len of 201407... 328755\n",
      "Len of cleaned 201407... 117369\n",
      "Processed 201407 in 0.68 seconds.\n",
      "Processing 201408...\n",
      "Len of 201408... 313679\n",
      "Len of cleaned 201408... 111756\n",
      "Processed 201408 in 0.60 seconds.\n",
      "Processing 201409...\n",
      "Len of 201409... 307465\n",
      "Len of cleaned 201409... 110043\n",
      "Processed 201409 in 0.60 seconds.\n",
      "Processing 201410...\n",
      "Len of 201410... 345309\n",
      "Len of cleaned 201410... 123243\n",
      "Processed 201410 in 0.67 seconds.\n",
      "Processing 201411...\n",
      "Len of 201411... 283639\n",
      "Len of cleaned 201411... 101418\n",
      "Processed 201411 in 0.54 seconds.\n",
      "Processing 201412...\n",
      "Len of 201412... 231949\n",
      "Len of cleaned 201412... 83810\n",
      "Processed 201412 in 0.43 seconds.\n",
      "Processing 201301...\n",
      "Len of 201301... 47971\n",
      "Len of cleaned 201301... 18125\n",
      "Processed 201301 in 0.11 seconds.\n",
      "Processing 201302...\n",
      "Len of 201302... 50327\n",
      "Len of cleaned 201302... 18958\n",
      "Processed 201302 in 0.10 seconds.\n",
      "Processing 201303...\n",
      "Len of 201303... 60210\n",
      "Len of cleaned 201303... 22731\n",
      "Processed 201303 in 0.13 seconds.\n",
      "Processing 201304...\n",
      "Len of 201304... 80746\n",
      "Len of cleaned 201304... 30805\n",
      "Processed 201304 in 0.17 seconds.\n",
      "Processing 201305...\n",
      "Len of 201305... 87461\n",
      "Len of cleaned 201305... 33279\n",
      "Processed 201305 in 0.18 seconds.\n",
      "Processing 201306...\n",
      "Len of 201306... 88999\n",
      "Len of cleaned 201306... 33725\n",
      "Processed 201306 in 0.18 seconds.\n",
      "Processing 201307...\n",
      "Len of 201307... 183894\n",
      "Len of cleaned 201307... 67396\n",
      "Processed 201307 in 0.36 seconds.\n",
      "Processing 201308...\n",
      "Len of 201308... 167506\n",
      "Len of cleaned 201308... 61465\n",
      "Processed 201308 in 0.34 seconds.\n",
      "Processing 201309...\n",
      "Len of 201309... 172402\n",
      "Len of cleaned 201309... 63058\n",
      "Processed 201309 in 0.47 seconds.\n",
      "Processing 201310...\n",
      "Len of 201310... 190588\n",
      "Len of cleaned 201310... 69320\n",
      "Processed 201310 in 0.40 seconds.\n",
      "Processing 201311...\n",
      "Len of 201311... 155801\n",
      "Len of cleaned 201311... 57223\n",
      "Processed 201311 in 0.32 seconds.\n",
      "Processing 201312...\n",
      "Len of 201312... 128209\n",
      "Len of cleaned 201312... 47668\n",
      "Processed 201312 in 0.26 seconds.\n",
      "Processing 201201...\n",
      "Len of 201201... 35\n",
      "Len of cleaned 201201... 17\n",
      "Processed 201201 in 0.01 seconds.\n",
      "Processing 201202...\n",
      "Len of 201202... 33\n",
      "Len of cleaned 201202... 18\n",
      "Processed 201202 in 0.01 seconds.\n",
      "Processing 201203...\n",
      "Len of 201203... 45\n",
      "Len of cleaned 201203... 22\n",
      "Processed 201203 in 0.01 seconds.\n",
      "Processing 201204...\n",
      "Len of 201204... 26\n",
      "Len of cleaned 201204... 12\n",
      "Processed 201204 in 0.01 seconds.\n",
      "Processing 201205...\n",
      "Len of 201205... 84\n",
      "Len of cleaned 201205... 37\n",
      "Processed 201205 in 0.01 seconds.\n",
      "Processing 201206...\n",
      "Len of 201206... 103\n",
      "Len of cleaned 201206... 38\n",
      "Processed 201206 in 0.01 seconds.\n",
      "Processing 201207...\n",
      "Len of 201207... 42\n",
      "Len of cleaned 201207... 18\n",
      "Processed 201207 in 0.01 seconds.\n",
      "Processing 201208...\n",
      "Len of 201208... 109\n",
      "Len of cleaned 201208... 47\n",
      "Processed 201208 in 0.00 seconds.\n",
      "Processing 201209...\n",
      "Len of 201209... 73\n",
      "Len of cleaned 201209... 31\n",
      "Processed 201209 in 0.01 seconds.\n",
      "Processing 201210...\n",
      "Len of 201210... 171\n",
      "Len of cleaned 201210... 70\n",
      "Processed 201210 in 0.01 seconds.\n",
      "Processing 201211...\n",
      "Len of 201211... 421\n",
      "Len of cleaned 201211... 166\n",
      "Processed 201211 in 0.01 seconds.\n",
      "Processing 201212...\n",
      "Len of 201212... 1074\n",
      "Len of cleaned 201212... 407\n",
      "Processed 201212 in 0.01 seconds.\n",
      "Completed in 105.30 seconds.\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_types(types_path):\n",
    "    return pl.read_csv(types_path)\n",
    "\n",
    "def load_fact_table(fact_path):\n",
    "    return pl.read_csv(fact_path)\n",
    "\n",
    "def build_parent_map(types_df):\n",
    "    parent_map = defaultdict(set)\n",
    "    for row in types_df.iter_rows(named=True):\n",
    "        tid = row['id']\n",
    "        for pid in (row['parent_id1'], row['parent_id2'], row['parent_id3']):\n",
    "            if pid > 0:\n",
    "                parent_map[tid].add(pid)\n",
    "    return parent_map\n",
    "\n",
    "def build_ancestor_map(parent_map):\n",
    "    ancestor_map = defaultdict(set)\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        for child, parents in parent_map.items():\n",
    "            current_ancestors = ancestor_map[child]\n",
    "            new_ancestors = set()\n",
    "            for parent in parents:\n",
    "                new_ancestors.add(parent)\n",
    "                new_ancestors.update(ancestor_map[parent])\n",
    "            if not new_ancestors.issubset(current_ancestors):\n",
    "                ancestor_map[child].update(new_ancestors)\n",
    "                changed = True\n",
    "    return dict(ancestor_map)\n",
    "\n",
    "def explode_types_column(fact_df: pl.DataFrame) -> pl.DataFrame:\n",
    "    return fact_df.select([\n",
    "        pl.col(\"id\"),\n",
    "        pl.col(\"x\").alias(\"Longitude\"),\n",
    "        pl.col(\"y\").alias(\"Latitude\"),\n",
    "        \"mp\", \"date\", \"state\", \"relevance\",\n",
    "        pl.col(\"types\")\n",
    "    ])\n",
    "\n",
    "def filter_most_granular_py(group, type_ancestors):\n",
    "    type_ids = set(group)\n",
    "    to_remove = set()\n",
    "    for t in type_ids:\n",
    "        to_remove.update(type_ids & type_ancestors.get(t, set()))\n",
    "    return [t not in to_remove for t in group]\n",
    "\n",
    "def get_granular_type_mask(fact_df: pl.DataFrame, type_ancestors: dict) -> pl.Series:\n",
    "    ids = fact_df[\"id\"].to_list()\n",
    "    types = fact_df[\"types\"].to_list()\n",
    "\n",
    "    granular_mask = []\n",
    "    current_id = None\n",
    "    group = []\n",
    "\n",
    "    for i in range(len(ids)):\n",
    "        if ids[i] != current_id:\n",
    "            if group:\n",
    "                mask = filter_most_granular_py(group, type_ancestors)\n",
    "                granular_mask.extend(mask)\n",
    "            group = [types[i]]\n",
    "            current_id = ids[i]\n",
    "        else:\n",
    "            group.append(types[i])\n",
    "    if group:\n",
    "        mask = filter_most_granular_py(group, type_ancestors)\n",
    "        granular_mask.extend(mask)\n",
    "\n",
    "    return pl.Series(\"granular\", granular_mask)\n",
    "\n",
    "def clean_fact_table_polars(fact_df: pl.DataFrame, type_ancestors: dict) -> pl.DataFrame:\n",
    "    exploded = explode_types_column(fact_df)\n",
    "    mask = get_granular_type_mask(exploded, type_ancestors)\n",
    "    filtered_df = exploded.with_columns(mask).filter(pl.col(\"granular\")).drop(\"granular\")\n",
    "\n",
    "    filtered_df = (\n",
    "    filtered_df.sort([\"id\", \"types\"])\n",
    "    .with_columns([\n",
    "        (pl.arange(0, pl.count()).over(\"id\")).alias(\"is_one\") + 1\n",
    "    ])\n",
    ")\n",
    "    filtered_df = filtered_df.with_columns([\n",
    "    pl.col(\"date\").str.strptime(pl.Datetime, format=\"%Y-%m-%dT%H:%M:%S%.f%z\").alias(\"parsed_date\"),\n",
    "    pl.col(\"date\").str.split(\"T\").list.get(0).alias(\"date_key\"),\n",
    "    pl.col(\"date\").str.split(\"T\").list.get(1).str.split(\":\").list.slice(0, 2).list.join(\":\").alias(\"time_key\"),\n",
    "])\n",
    "\n",
    "    # Drop the original 'date' column if no longer needed\n",
    "    filtered_df = filtered_df.drop(\"date\", \"parsed_date\")\n",
    "\n",
    "\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "def save_fact_table(df: pl.DataFrame, output_path: str):\n",
    "    df.write_csv(output_path)\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "\n",
    "    types_path = \"C://Users//jirip//Documents//Developer//python//kriminalita//dimensions//types.csv\"\n",
    "\n",
    "    yearMonths = ['202501','202502','202503','202504','202401','202402','202403','202404','202405','202406','202407','202408','202409','202410','202411','202412','202301','202302','202303','202304','202305','202306','202307','202308','202309','202310','202311','202312','202201','202202','202203','202204','202205','202206','202207','202208','202209','202210','202211','202212','202101','202102','202103','202104','202105','202106','202107','202108','202109','202110','202111','202112','202001','202002','202003','202004','202005','202006','202007','202008','202009','202010','202011','202012','201901','201902','201903','201904','201905','201906','201907','201908','201909','201910','201911','201912','201801','201802','201803','201804','201805','201806','201807','201808','201809','201810','201811','201812','201701','201702','201703','201704','201705','201706','201707','201708','201709','201710','201711','201712','201601','201602','201603','201604','201605','201606','201607','201608','201609','201610','201611','201612','201501','201502','201503','201504','201505','201506','201507','201508','201509','201510','201511','201512','201401','201402','201403','201404','201405','201406','201407','201408','201409','201410','201411','201412','201301','201302','201303','201304','201305','201306','201307','201308','201309','201310','201311','201312','201201','201202','201203','201204','201205','201206','201207','201208','201209','201210','201211','201212']\n",
    "    # yearMonths = ['202501','202502']\n",
    "\n",
    "\n",
    "    for yearMonth in yearMonths:\n",
    "        loop_start_time = time.time()\n",
    "        fact_path = f\"C://Users//jirip//Documents//Developer//python//kriminalita//source_files//{yearMonth}.csv\"\n",
    "        output_path = f\"processed_files//fact_clean_{yearMonth}.csv\"\n",
    "        print(f\"Processing {yearMonth}...\")\n",
    "\n",
    "        types_df = load_types(types_path)\n",
    "        fact_df = load_fact_table(fact_path)\n",
    "        print(f\"Len of {yearMonth}... {len(fact_df)}\")\n",
    "\n",
    "        parent_map = build_parent_map(types_df)\n",
    "        type_ancestors = build_ancestor_map(parent_map)\n",
    "\n",
    "        cleaned_df = clean_fact_table_polars(fact_df, type_ancestors)\n",
    "        print(f\"Len of cleaned {yearMonth}... {len(cleaned_df)}\")\n",
    "\n",
    "        save_fact_table(cleaned_df, output_path)\n",
    "        elapsed = time.time() - loop_start_time\n",
    "        print(f\"Processed {yearMonth} in {elapsed:.2f} seconds.\")  \n",
    "\n",
    "    print(f\"Completed in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4df5a86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 10)\n",
      "┌──────┬───────────┬───────────┬───────┬───┬───────┬────────┬────────────┬───────────┐\n",
      "│ id   ┆ Longitude ┆ Latitude  ┆ mp    ┆ … ┆ types ┆ is_one ┆ date_only  ┆ time_only │\n",
      "│ ---  ┆ ---       ┆ ---       ┆ ---   ┆   ┆ ---   ┆ ---    ┆ ---        ┆ ---       │\n",
      "│ i64  ┆ f64       ┆ f64       ┆ bool  ┆   ┆ i64   ┆ i64    ┆ str        ┆ str       │\n",
      "╞══════╪═══════════╪═══════════╪═══════╪═══╪═══════╪════════╪════════════╪═══════════╡\n",
      "│ 739  ┆ 17.316089 ┆ 49.30221  ┆ false ┆ … ┆ 101   ┆ 1      ┆ 2012-01-19 ┆ 13:14     │\n",
      "│ 739  ┆ 17.316089 ┆ 49.30221  ┆ false ┆ … ┆ 102   ┆ 2      ┆ 2012-01-19 ┆ 13:14     │\n",
      "│ 1075 ┆ 14.585108 ┆ 50.226063 ┆ false ┆ … ┆ 120   ┆ 1      ┆ 2012-01-26 ┆ 16:00     │\n",
      "│ 2822 ┆ 17.251485 ┆ 49.572761 ┆ false ┆ … ┆ 77    ┆ 1      ┆ 2012-01-27 ┆ 10:03     │\n",
      "│ 3016 ┆ 15.796849 ┆ 49.957659 ┆ false ┆ … ┆ 54    ┆ 1      ┆ 2012-01-12 ┆ 09:20     │\n",
      "└──────┴───────────┴───────────┴───────┴───┴───────┴────────┴────────────┴───────────┘\n",
      "17764377\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = \"C://Users//jirip//Documents//Developer//python//kriminalita//processed_files\"\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "# Read and append all files\n",
    "appended_df = pl.concat([pl.read_csv(file) for file in csv_files])\n",
    "\n",
    "# Preview the appended DataFrame\n",
    "print(appended_df.head())\n",
    "print(len(appended_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0efedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape, Point\n",
    "\n",
    "# Load obce polygons from JSON\n",
    "with open('C://Users//jirip//Documents//Developer//python//kriminalita//source_files//okresy.json') as f:\n",
    "    regions_data = json.load(f)\n",
    "\n",
    "# Prepare polygons\n",
    "regions = []\n",
    "for feature in regions_data['features']:  # assuming GeoJSON-like structure\n",
    "    region_id = feature['id']\n",
    "    polygon = shape(feature['geometry'])  # shapely geometry from geojson\n",
    "    regions.append((region_id, polygon))\n",
    "    \n",
    "#  Load your obce polygons from JSON\n",
    "with open('C://Users//jirip//Documents//Developer//python//kriminalita//source_files//obce.json') as f:\n",
    "    cities_data = json.load(f)\n",
    "# Prepare polygons\n",
    "cities = []\n",
    "for feature in cities_data['features']:  # assuming GeoJSON-like structure\n",
    "    city_id = feature['id']\n",
    "    polygon = shape(feature['geometry'])  # shapely geometry from geojson\n",
    "    cities.append((city_id, polygon))\n",
    "\n",
    "df = appended_df.to_pandas()\n",
    "\n",
    "# Prepare regions GeoDataFrame\n",
    "gdf_regions = gpd.GeoDataFrame({'region_id': [r[0] for r in regions]}, geometry=[r[1] for r in regions])\n",
    "\n",
    "# Prepare points GeoDataFrame\n",
    "gdf_points = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.Longitude, df.Latitude))\n",
    "\n",
    "# Spatial join\n",
    "gdf_joined = gpd.sjoin(gdf_points, gdf_regions, how='left', predicate='within')\n",
    "\n",
    "# If needed, bring the result back to pandas\n",
    "df_result = pd.DataFrame(gdf_joined.drop(columns=['geometry', 'index_right']))\n",
    "# df_result['mp'] = df_result['mp'].astype(int)\n",
    "# df_result['region_id'] = df_result['region_id'].str[:-6]\n",
    "\n",
    "# Prepare cities GeoDataFrame\n",
    "gdf_cities = gpd.GeoDataFrame({'city_id': [r[0] for r in cities]}, geometry=[r[1] for r in cities])\n",
    "gdf_points = gpd.GeoDataFrame(df_result, geometry=gpd.points_from_xy(df_result.Longitude, df_result.Latitude))\n",
    "gdf_joined = gpd.sjoin(gdf_points, gdf_cities,how='left', predicate='within')\n",
    "df_result_final = pd.DataFrame(gdf_joined.drop(columns=['geometry', 'index_right', 'Longitude', 'Latitude']))\n",
    "df_result_final['mp'] = df_result_final['mp'].astype(int)\n",
    "df_result_final['city_id'] = df_result_final['city_id'].fillna(df_result_final['region_id'])\n",
    "df_result_final = df_result_final.drop(columns=['region_id'])\n",
    "df_result_final.to_csv('mapped_files//F_Crime.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10370f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18433166\n"
     ]
    }
   ],
   "source": [
    "print(len(df_result_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a0a3f3",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Split the 'date' column into separate 'date' and 'time' columns\n",
    "appended_df = appended_df.with_columns([\n",
    "    pl.col(\"date\").str.strptime(pl.Datetime, format=\"%Y-%m-%dT%H:%M:%S%.f%z\").alias(\"parsed_date\"),\n",
    "    pl.col(\"date\").str.split(\"T\").list.get(0).alias(\"date_only\"),\n",
    "    pl.col(\"date\").str.split(\"T\").list.get(1).str.split(\":00.\").list.get(0).alias(\"time_only\"),\n",
    "])\n",
    "\n",
    "# Drop the original 'date' column if no longer needed\n",
    "appended_df = appended_df.drop(\"date\", \"parsed_date\")\n",
    "\n",
    "# Preview the updated DataFrame\n",
    "print(appended_df.head())\n",
    "\n",
    "# Save the appended DataFrame to a new CSV file\n",
    "appended_df.write_csv(\"processed_files//appended_fact_clean.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
