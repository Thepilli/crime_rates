{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c63d810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load types\n",
    "types_df = pd.read_csv(\n",
    "    \"C://Users//jirip//Documents//Developer//python//kriminalita//types.csv\",\n",
    "    sep=\",\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "# Build direct parent map\n",
    "parent_map = (\n",
    "    types_df\n",
    "    .melt(id_vars='id', value_vars=['parent_id1', 'parent_id2', 'parent_id3'], value_name='parent_id')\n",
    "    .dropna(subset=['parent_id'])\n",
    "    .query('parent_id > 0')\n",
    "    .groupby('id')['parent_id']\n",
    "    .apply(set)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Initialize ancestor map\n",
    "ancestor_map = {tid: set() for tid in types_df['id']}\n",
    "\n",
    "# Iteratively propagate ancestors\n",
    "updated = True\n",
    "while updated:\n",
    "    updated = False\n",
    "    for child, parents in parent_map.items():\n",
    "        current_ancestors = ancestor_map[child].copy()\n",
    "        for parent in parents:\n",
    "            current_ancestors.add(parent)\n",
    "            current_ancestors.update(ancestor_map[parent])  # inherit ancestors of parent\n",
    "        if current_ancestors != ancestor_map[child]:\n",
    "            ancestor_map[child] = current_ancestors\n",
    "            updated = True  # Keep looping if anything changed\n",
    "\n",
    "# Now ancestor_map holds: {type_id: set(all ancestors)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90cd1303",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jirip\\AppData\\Local\\Temp\\ipykernel_3960\\4040474647.py:108: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(filter_most_granular)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  Longitude   Latitude     mp                            date  \\\n",
      "0  26505406  14.414338  50.087987  False  2025-04-04T13:00:00.0000+02:00   \n",
      "1  26767015  17.693409  49.080685  False  2025-04-11T20:14:00.0000+02:00   \n",
      "2  26863079  14.195408  50.743945  False  2025-04-01T12:32:00.0000+02:00   \n",
      "3  26863079  14.195408  50.743945  False  2025-04-01T12:32:00.0000+02:00   \n",
      "4  26896576  14.422167  50.066555  False  2025-04-04T00:47:00.0000+02:00   \n",
      "\n",
      "   state  relevance  types  is_one  \n",
      "0      2          4    102       1  \n",
      "1      1          4    111       1  \n",
      "2      2          4     69       1  \n",
      "3      2          4    127       2  \n",
      "4      2          4      5       1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# === 1. Load and Prepare Types Data ===\n",
    "types_df = pd.read_csv(\n",
    "    \"C://Users//jirip//Documents//Developer//python//kriminalita//types.csv\",\n",
    "    sep=\",\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "type_lookup = types_df.set_index('id')[['name', 'label']].to_dict('index')\n",
    "\n",
    "def expand_hierarchy(row):\n",
    "    ids = [row['parent_id1'], row['parent_id2'], row['parent_id3'], row['id']]\n",
    "    labels = [type_lookup.get(i, {'label': None})['label'] if i > 0 else None for i in ids]\n",
    "    names = [type_lookup.get(i, {'name': None})['name'] if i > 0 else None for i in ids]\n",
    "\n",
    "    result = {}\n",
    "    for level, (tid, label, name) in enumerate(zip(ids, labels, names), 1):\n",
    "        result[f'level_{level}_id'] = tid if tid > 0 else None\n",
    "        result[f'level_{level}_label'] = label\n",
    "        result[f'level_{level}_name'] = name\n",
    "    return pd.Series(result)\n",
    "\n",
    "dimension_df = types_df.apply(expand_hierarchy, axis=1)\n",
    "dimension_df = pd.concat([types_df[['id']], dimension_df, types_df[['name', 'label']]], axis=1)\n",
    "\n",
    "# Save dimension table\n",
    "dimension_df.to_csv('dim_type.csv', index=False)\n",
    "\n",
    "# === 2. Build Type Ancestor Map ===\n",
    "# Build direct parent map\n",
    "parent_map = (\n",
    "    types_df\n",
    "    .melt(id_vars='id', value_vars=['parent_id1', 'parent_id2', 'parent_id3'], value_name='parent_id')\n",
    "    .dropna(subset=['parent_id'])\n",
    "    .query('parent_id > 0')\n",
    "    .groupby('id')['parent_id']\n",
    "    .apply(set)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Initialize ancestor map\n",
    "ancestor_map = {tid: set() for tid in types_df['id']}\n",
    "\n",
    "# Iteratively propagate ancestors\n",
    "updated = True\n",
    "while updated:\n",
    "    updated = False\n",
    "    for child, parents in parent_map.items():\n",
    "        current_ancestors = ancestor_map[child].copy()\n",
    "        for parent in parents:\n",
    "            current_ancestors.add(parent)\n",
    "            current_ancestors.update(ancestor_map[parent])  # inherit ancestors of parent\n",
    "        if current_ancestors != ancestor_map[child]:\n",
    "            ancestor_map[child] = current_ancestors\n",
    "            updated = True  # Keep looping if anything changed\n",
    "\n",
    "# Now ancestor_map holds: {type_id: set(all ancestors)}\n",
    "\n",
    "\n",
    "type_ancestors = ancestor_map\n",
    "\n",
    "# === 3. Load Fact Data ===\n",
    "fact_df = pd.read_csv(\n",
    "    \"C://Users//jirip//Documents//Developer//python//kriminalita//202504.csv\",\n",
    "    sep=\",\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "# === 4. Filter Most Granular Types ===\n",
    "def filter_most_granular(group):\n",
    "    type_ids = set(group['types'])\n",
    "\n",
    "    # Remove non-granular types\n",
    "    to_remove = set()\n",
    "    for t in type_ids:\n",
    "        ancestors = type_ancestors.get(t, set())\n",
    "        to_remove.update(type_ids & ancestors)  # Only care if ancestors exist in current types\n",
    "\n",
    "    granular_types = type_ids - to_remove\n",
    "\n",
    "    # Take common attributes once\n",
    "    row = group.iloc[0]\n",
    "    common_data = {\n",
    "        'id': row['id'],\n",
    "        'Longitude': row['x'],\n",
    "        'Latitude': row['y'],\n",
    "        'mp': row['mp'],\n",
    "        'date': row['date'],\n",
    "        'state': row['state'],\n",
    "        'relevance': row['relevance'],\n",
    "    }\n",
    "\n",
    "    # Expand into rows\n",
    "    records = [\n",
    "        {**common_data, 'types': t}\n",
    "        for t in granular_types\n",
    "    ]\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "# === 5. Process and Save ===\n",
    "filtered_fact_df = (\n",
    "    fact_df\n",
    "    .groupby('id', group_keys=False)\n",
    "    .apply(filter_most_granular)\n",
    "    .reset_index(drop=True)\n",
    "    .sort_values(by=['id', 'types'])\n",
    "    .assign(is_one=lambda df: df.groupby('id').cumcount() + 1)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "filtered_fact_df.to_csv('fact_clean.csv', index=False)\n",
    "\n",
    "# Preview the cleaned fact table\n",
    "print(filtered_fact_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756cfe62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jirip\\AppData\\Local\\Temp\\ipykernel_3960\\3799056573.py:85: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(filter_most_granular)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  types  Longitude   Latitude     mp  \\\n",
      "0  26505406    102  14.414338  50.087987  False   \n",
      "1  26767015    111  17.693409  49.080685  False   \n",
      "2  26863079     69  14.195408  50.743945  False   \n",
      "3  26863079    127  14.195408  50.743945  False   \n",
      "4  26896576      5  14.422167  50.066555  False   \n",
      "\n",
      "                             date  state  relevance  is_one  \n",
      "0  2025-04-04T13:00:00.0000+02:00      2          4       1  \n",
      "1  2025-04-11T20:14:00.0000+02:00      1          4       1  \n",
      "2  2025-04-01T12:32:00.0000+02:00      2          4       1  \n",
      "3  2025-04-01T12:32:00.0000+02:00      2          4       2  \n",
      "4  2025-04-04T00:47:00.0000+02:00      2          4       1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# === 1. Load and Prepare Types Data ===\n",
    "types_df = pd.read_csv(\n",
    "    \"C://Users//jirip//Documents//Developer//python//kriminalita//types.csv\",\n",
    "    sep=\",\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "type_lookup = types_df.set_index('id')[['name', 'label']].to_dict('index')\n",
    "\n",
    "def expand_hierarchy(row):\n",
    "    ids = [row['parent_id1'], row['parent_id2'], row['parent_id3'], row['id']]\n",
    "    labels = [type_lookup.get(i, {'label': None})['label'] if i > 0 else None for i in ids]\n",
    "    names = [type_lookup.get(i, {'name': None})['name'] if i > 0 else None for i in ids]\n",
    "\n",
    "    result = {}\n",
    "    for level, (tid, label, name) in enumerate(zip(ids, labels, names), 1):\n",
    "        result[f'level_{level}_id'] = tid if tid > 0 else None\n",
    "        result[f'level_{level}_label'] = label\n",
    "        result[f'level_{level}_name'] = name\n",
    "    return pd.Series(result)\n",
    "\n",
    "dimension_df = types_df.apply(expand_hierarchy, axis=1)\n",
    "dimension_df = pd.concat([types_df[['id']], dimension_df, types_df[['name', 'label']]], axis=1)\n",
    "\n",
    "# Save dimension table\n",
    "dimension_df.to_csv('dim_type_new.csv', index=False)\n",
    "\n",
    "# === 2. Build Type Ancestor Map ===\n",
    "def build_type_ancestor_map(df: pd.DataFrame) -> dict[int, set[int]]:\n",
    "    def collect_ancestors(type_id: int) -> set[int]:\n",
    "        if not type_id or pd.isna(type_id):\n",
    "            return set()\n",
    "\n",
    "        row = df.loc[df['id'] == type_id]\n",
    "        if row.empty:\n",
    "            return set()\n",
    "\n",
    "        direct_parents = {pid for pid in row[['parent_id1', 'parent_id2', 'parent_id3']].values.flatten() if pid > 0}\n",
    "\n",
    "        ancestors = set(direct_parents)\n",
    "        for parent in direct_parents:\n",
    "            ancestors |= collect_ancestors(parent)\n",
    "\n",
    "        return ancestors\n",
    "\n",
    "    return {tid: collect_ancestors(tid) for tid in df['id']}\n",
    "\n",
    "type_ancestors = build_type_ancestor_map(types_df)\n",
    "\n",
    "# === 3. Load Fact Data ===\n",
    "fact_df = pd.read_csv(\n",
    "    \"C://Users//jirip//Documents//Developer//python//kriminalita//202504.csv\",\n",
    "    sep=\",\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "# === 4. Filter Most Granular Types ===\n",
    "def filter_most_granular(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    type_ids = set(group['types'])\n",
    "    to_remove = {\n",
    "        t1 for t1 in type_ids\n",
    "        for t2 in type_ids\n",
    "        if t1 != t2 and t1 in type_ancestors.get(t2, set())\n",
    "    }\n",
    "    granular_types = type_ids - to_remove\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'id': [group['id'].iloc[0]] * len(granular_types),\n",
    "        'types': list(granular_types),\n",
    "        'Longitude': group['x'].iloc[0],\n",
    "        'Latitude': group['y'].iloc[0],\n",
    "        'mp': group['mp'].iloc[0],\n",
    "        'date': group['date'].iloc[0],\n",
    "        'state': group['state'].iloc[0],\n",
    "        'relevance': group['relevance'].iloc[0]\n",
    "    })\n",
    "\n",
    "# === 5. Process and Save ===\n",
    "filtered_fact_df = (\n",
    "    fact_df\n",
    "    .groupby('id', group_keys=False)\n",
    "    .apply(filter_most_granular)\n",
    "    .reset_index(drop=True)\n",
    "    .sort_values(by=['id', 'types'])\n",
    "    .assign(is_one=lambda df: df.groupby('id').cumcount() + 1)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "filtered_fact_df.to_csv('fact_clean_new.csv', index=False)\n",
    "\n",
    "# Preview the cleaned fact table\n",
    "print(filtered_fact_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e469237d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 202504...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jirip\\AppData\\Local\\Temp\\ipykernel_22624\\3449544001.py:71: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  filtered_df = fact_df.groupby('id', group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 202504 in 63.49 seconds.\n",
      "Processing 202503...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jirip\\AppData\\Local\\Temp\\ipykernel_22624\\3449544001.py:71: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  filtered_df = fact_df.groupby('id', group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 202503 in 152.04 seconds.\n",
      "Processing 202502...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jirip\\AppData\\Local\\Temp\\ipykernel_22624\\3449544001.py:71: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  filtered_df = fact_df.groupby('id', group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 202502 in 241.82 seconds.\n",
      "Processing 202501...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jirip\\AppData\\Local\\Temp\\ipykernel_22624\\3449544001.py:71: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  filtered_df = fact_df.groupby('id', group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 202501 in 333.81 seconds.\n",
      "Completed in 333.81 seconds.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_types(types_path):\n",
    "    types_df = pd.read_csv(types_path, sep=\",\", encoding=\"utf-8\")\n",
    "    return types_df\n",
    "\n",
    "def build_parent_map(types_df):\n",
    "    parent_map = defaultdict(set)\n",
    "    for _, row in types_df.iterrows():\n",
    "        tid = row['id']\n",
    "        for pid in (row['parent_id1'], row['parent_id2'], row['parent_id3']):\n",
    "            if pid > 0:\n",
    "                parent_map[tid].add(pid)\n",
    "    return parent_map\n",
    "\n",
    "def build_ancestor_map(parent_map):\n",
    "    ancestor_map = defaultdict(set)\n",
    "\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        for child, parents in parent_map.items():\n",
    "            current_ancestors = ancestor_map[child]\n",
    "            new_ancestors = set()\n",
    "            for parent in parents:\n",
    "                new_ancestors.add(parent)\n",
    "                new_ancestors.update(ancestor_map[parent])\n",
    "            if not new_ancestors.issubset(current_ancestors):\n",
    "                ancestor_map[child].update(new_ancestors)\n",
    "                changed = True\n",
    "    return dict(ancestor_map)\n",
    "\n",
    "def load_fact_table(fact_path):\n",
    "    fact_df = pd.read_csv(fact_path, sep=\",\", encoding=\"utf-8\")\n",
    "    return fact_df\n",
    "\n",
    "def filter_most_granular(group, type_ancestors):\n",
    "    type_ids = set(group['types'])\n",
    "\n",
    "    # Remove types that are ancestors of others\n",
    "    to_remove = set()\n",
    "    for t in type_ids:\n",
    "        ancestors = type_ancestors.get(t, set())\n",
    "        to_remove.update(type_ids & ancestors)\n",
    "\n",
    "    granular_types = type_ids - to_remove\n",
    "\n",
    "    # Fixed attributes\n",
    "    row = group.iloc[0]\n",
    "    common_data = {\n",
    "        'id': row['id'],\n",
    "        'Longitude': row['x'],\n",
    "        'Latitude': row['y'],\n",
    "        'mp': row['mp'],\n",
    "        'date': row['date'],\n",
    "        'state': row['state'],\n",
    "        'relevance': row['relevance'],\n",
    "    }\n",
    "\n",
    "    # Expand\n",
    "    records = [\n",
    "        {**common_data, 'types': t}\n",
    "        for t in granular_types\n",
    "    ]\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "def clean_fact_table(fact_df, type_ancestors):\n",
    "    # Group and apply filtering\n",
    "    filtered_df = fact_df.groupby('id', group_keys=False).apply(\n",
    "        lambda group: filter_most_granular(group, type_ancestors)\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # Assign 'is_one'\n",
    "    filtered_df['is_one'] = (\n",
    "        filtered_df.sort_values(by=['id', 'types'])\n",
    "        .groupby('id')\n",
    "        .cumcount() + 1\n",
    "    )\n",
    "\n",
    "    return filtered_df.sort_values(by=['id', 'types']).reset_index(drop=True)\n",
    "\n",
    "def save_fact_table(filtered_df, output_path):\n",
    "    filtered_df.to_csv(output_path, index=False)\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Paths\n",
    "    types_path = \"C://Users//jirip//Documents//Developer//python//kriminalita//types.csv\"\n",
    "    \n",
    "    yearMonth = [\"202504\",\"202503\",\"202502\",\"202501\"]\n",
    "    for year in yearMonth:\n",
    "        loop_start_time = time.time()\n",
    "        fact_path = f\"C://Users//jirip//Documents//Developer//python//kriminalita//map_files//{year}.csv\"\n",
    "        output_path = f\"fact_clean_{year}.csv\"\n",
    "        print(f\"Processing {year}...\")\n",
    "        # Load\n",
    "        types_df = load_types(types_path)\n",
    "        fact_df = load_fact_table(fact_path)\n",
    "\n",
    "        # Build maps\n",
    "        parent_map = build_parent_map(types_df)\n",
    "        type_ancestors = build_ancestor_map(parent_map)\n",
    "\n",
    "        # Clean fact table\n",
    "        cleaned_fact_df = clean_fact_table(fact_df, type_ancestors)\n",
    "\n",
    "        # Save\n",
    "        save_fact_table(cleaned_fact_df, output_path)\n",
    "        \n",
    "        elapsed = time.time() - loop_start_time\n",
    "        print(f\"Processed {year} in {elapsed:.2f} seconds.\")    \n",
    "\n",
    "\n",
    "    \n",
    "    # Print elapsed time\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Completed in {elapsed:.2f} seconds.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dc97f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jirip\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'parallel_filter.<locals>.<lambda>'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 77\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mconcat(results)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m filtered_df \u001b[38;5;241m=\u001b[39m \u001b[43mparallel_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerged_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# === Step 6: Mark Single/Multiple Types ===\u001b[39;00m\n\u001b[0;32m     80\u001b[0m filtered_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_one\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     81\u001b[0m     filtered_df\u001b[38;5;241m.\u001b[39msort_values([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtypes\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcumcount() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     83\u001b[0m )\n",
      "Cell \u001b[1;32mIn[4], line 73\u001b[0m, in \u001b[0;36mparallel_filter\u001b[1;34m(df, num_processes)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mp\u001b[38;5;241m.\u001b[39mPool(processes\u001b[38;5;241m=\u001b[39mnum_processes) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m     72\u001b[0m     parts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray_split(df, num_processes)\n\u001b[1;32m---> 73\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilter_granular\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterrows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mconcat(results)\n",
      "File \u001b[1;32mc:\\Users\\jirip\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[1;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jirip\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[1;32mc:\\Users\\jirip\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\pool.py:540\u001b[0m, in \u001b[0;36mPool._handle_tasks\u001b[1;34m(taskqueue, put, outqueue, pool, cache)\u001b[0m\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 540\u001b[0m     \u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    542\u001b[0m     job, idx \u001b[38;5;241m=\u001b[39m task[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\jirip\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\connection.py:206\u001b[0m, in \u001b[0;36m_ConnectionBase.send\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_writable()\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_bytes(\u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\jirip\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\reduction.py:51\u001b[0m, in \u001b[0;36mForkingPickler.dumps\u001b[1;34m(cls, obj, protocol)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdumps\u001b[39m(\u001b[38;5;28mcls\u001b[39m, obj, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     50\u001b[0m     buf \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO()\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buf\u001b[38;5;241m.\u001b[39mgetbuffer()\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't pickle local object 'parallel_filter.<locals>.<lambda>'"
     ]
    }
   ],
   "source": [
    "# parallelize\n",
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import multiprocessing as mp\n",
    "\n",
    "# === Step 1: Load Type Hierarchy and Build Ancestor Map Iteratively ===\n",
    "types_df = pd.read_csv(\"C://Users//jirip//Documents//Developer//python//kriminalita//types.csv\", sep=\",\", encoding=\"utf-8\")\n",
    "\n",
    "def build_ancestor_map(types_df):\n",
    "    parent_map = defaultdict(set)\n",
    "    for _, row in types_df.iterrows():\n",
    "        tid = row['id']\n",
    "        for pid in (row['parent_id1'], row['parent_id2'], row['parent_id3']):\n",
    "            if pid > 0:\n",
    "                parent_map[tid].add(pid)\n",
    "\n",
    "    ancestor_map = defaultdict(set)\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        for child, parents in parent_map.items():\n",
    "            current = ancestor_map[child]\n",
    "            new_ancestors = set()\n",
    "            for p in parents:\n",
    "                new_ancestors.add(p)\n",
    "                new_ancestors.update(ancestor_map[p])\n",
    "            if not new_ancestors.issubset(current):\n",
    "                ancestor_map[child].update(new_ancestors)\n",
    "                changed = True\n",
    "    return dict(ancestor_map)\n",
    "\n",
    "ancestor_map = build_ancestor_map(types_df)\n",
    "\n",
    "# === Step 2: Load and Prepare the Fact Table ===\n",
    "fact_df = pd.read_csv(\"C://Users//jirip//Documents//Developer//python//kriminalita//map_files//202504.csv\", dtype={'id': int, 'types': int})\n",
    "fact_df.rename(columns={'x': 'Longitude', 'y': 'Latitude'}, inplace=True)\n",
    "\n",
    "# === Step 3: Group Type IDs Per 'id' ===\n",
    "grouped = fact_df.groupby('id')\n",
    "meta_cols = ['Longitude', 'Latitude', 'mp', 'date', 'state', 'relevance']\n",
    "\n",
    "# Extract metadata per id\n",
    "meta_df = grouped[meta_cols].first().reset_index()\n",
    "\n",
    "# Extract types list per id\n",
    "types_grouped = grouped['types'].apply(list).reset_index(name='types')\n",
    "\n",
    "# Merge\n",
    "merged_df = pd.merge(meta_df, types_grouped, on='id')\n",
    "\n",
    "\n",
    "# === Step 4: Filter to Most Granular Types ===\n",
    "def filter_granular(row):\n",
    "    types = set(row['types'])\n",
    "    to_remove = set()\n",
    "    for t1 in types:\n",
    "        for t2 in types:\n",
    "            if t1 != t2 and t1 in ancestor_map.get(t2, set()):\n",
    "                to_remove.add(t1)\n",
    "    granular = types - to_remove\n",
    "    return pd.DataFrame({\n",
    "        'id': [row['id']] * len(granular),\n",
    "        'types': list(granular),\n",
    "        **{col: [row[col]] * len(granular) for col in meta_cols}\n",
    "    })\n",
    "\n",
    "\n",
    "# === Step 5: Parallel Processing (Optional but Recommended for Large Files) ===\n",
    "def parallel_filter(df, num_processes=mp.cpu_count()):\n",
    "    with mp.Pool(processes=num_processes) as pool:\n",
    "        parts = np.array_split(df, num_processes)\n",
    "        results = pool.map(lambda part: pd.concat([filter_granular(r) for _, r in part.iterrows()]), parts)\n",
    "    return pd.concat(results)\n",
    "\n",
    "import numpy as np\n",
    "filtered_df = parallel_filter(merged_df)\n",
    "\n",
    "# === Step 6: Mark Single/Multiple Types ===\n",
    "filtered_df['is_one'] = (\n",
    "    filtered_df.sort_values(['id', 'types'])\n",
    "    .groupby('id').cumcount() + 1\n",
    ")\n",
    "\n",
    "# === Step 7: Save Cleaned Data ===\n",
    "filtered_df.to_csv(\"fact_clean.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
