{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba1673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "# --- Simulate loading your source types CSV data ---\n",
    "# csv_data = \"\"\"\n",
    "# \"id\",\"parent_id1\",\"parent_id2\",\"parent_id3\",\"name\",\"label\"\n",
    "# 116,97,75,115,\"PŘ – alkohol ostatní\",\"Alkohol ostatní\"\n",
    "# 97,0,0,0,\"Přestupky\",\"Přestupky\"\n",
    "# 75,97,0,0,\"PŘ – alkohol a toxi\",\"Na úseku ochrany před alkoholismem toxikomanií\"\n",
    "# 115,97,75,0,\"PŘ – alkohol ostatní\",\"Alkohol ostatní\"\n",
    "# \"\"\"\n",
    "# df_source = pd.read_csv(io.StringIO(csv_data))\n",
    "df_source = pd.read_csv(\"C://Users//jirip//Documents//Developer//python//kriminalita//types.csv\", sep=\",\", encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "# Replace 0 parent IDs with NaN\n",
    "df_source['parent_id1'] = df_source['parent_id1'].replace(0, pd.NA)\n",
    "df_source['parent_id2'] = df_source['parent_id2'].replace(0, pd.NA)\n",
    "df_source['parent_id3'] = df_source['parent_id3'].replace(0, pd.NA)\n",
    "\n",
    "# --- Prepare the dimension data ---\n",
    "df_dim = df_source[['id', 'name', 'label', 'parent_id1', 'parent_id2', 'parent_id3']].copy()\n",
    "df_dim.rename(columns={\n",
    "    'id': 'Level4_ID', 'name': 'Level4_Name', 'label': 'Level4_Label'\n",
    "}, inplace=True)\n",
    "\n",
    "df_lookup = df_source[['id', 'name', 'label']].copy()\n",
    "\n",
    "# Merge to get Level 3, 2, 1 attributes (as before)\n",
    "df_dim = pd.merge(df_dim, df_lookup.rename(columns={'id': 'Level3_ID', 'name': 'Level3_Name', 'label': 'Level3_Label'}), left_on='parent_id3', right_on='Level3_ID', how='left')\n",
    "df_dim = pd.merge(df_dim, df_lookup.rename(columns={'id': 'Level2_ID', 'name': 'Level2_Name', 'label': 'Level2_Label'}), left_on='parent_id2', right_on='Level2_ID', how='left')\n",
    "df_dim = pd.merge(df_dim, df_lookup.rename(columns={'id': 'Level1_ID', 'name': 'Level1_Name', 'label': 'Level1_Label'}), left_on='parent_id1', right_on='Level1_ID', how='left')\n",
    "\n",
    "# Add SourceTypeID\n",
    "df_dim['SourceTypeID'] = df_dim['Level4_ID']\n",
    "\n",
    "# --- *** ADD TypeSK GENERATION HERE *** ---\n",
    "# Option A: Using reset_index (starts SK from 0)\n",
    "# df_dim = df_dim.reset_index()\n",
    "# df_dim.rename(columns={'index': 'TypeSK'}, inplace=True)\n",
    "\n",
    "# Option B: Creating a sequence (starts SK from 1 - often preferred for DW keys)\n",
    "df_dim.insert(0, 'TypeSK', range(1, len(df_dim) + 1))\n",
    "\n",
    "\n",
    "# --- Finalize the dimension DataFrame ---\n",
    "# Calculate optional fields (Depth, Path)\n",
    "def calculate_depth(row):\n",
    "    # Note: Adjusted depth calculation slightly to be more robust\n",
    "    depth = 0\n",
    "    if pd.notna(row['Level1_ID']) or pd.notna(row['Level1_Name']): depth = 1 # If L1 exists, depth is at least 1\n",
    "    if pd.notna(row['Level2_ID']) or pd.notna(row['Level2_Name']): depth = 2 # If L2 exists, depth is at least 2\n",
    "    if pd.notna(row['Level3_ID']) or pd.notna(row['Level3_Name']): depth = 3 # If L3 exists, depth is at least 3\n",
    "    if pd.notna(row['Level4_ID']) or pd.notna(row['Level4_Name']): depth = 4 # If L4 exists, depth is at least 4\n",
    "    # Handle root nodes correctly (where L1 is populated but others might be NA)\n",
    "    if depth == 0 and (pd.notna(row['Level4_ID']) or pd.notna(row['Level4_Name'])):\n",
    "        depth = 1 # If only L4 details exist somehow (e.g. root node entered weirdly) treat as depth 1\n",
    "    return depth if depth > 0 else 1 # Ensure minimum depth 1 if valid L4 exists\n",
    "\n",
    "\n",
    "def calculate_path(row, field='Name'):\n",
    "    parts = []\n",
    "    if pd.notna(row[f'Level1_{field}']): parts.append(str(row[f'Level1_{field}']))\n",
    "    if pd.notna(row[f'Level2_{field}']): parts.append(str(row[f'Level2_{field}']))\n",
    "    if pd.notna(row[f'Level3_{field}']): parts.append(str(row[f'Level3_{field}']))\n",
    "    if pd.notna(row[f'Level4_{field}']): parts.append(str(row[f'Level4_{field}']))\n",
    "    return ' > '.join(parts) if parts else None\n",
    "\n",
    "df_dim['HierarchyDepth'] = df_dim.apply(calculate_depth, axis=1)\n",
    "df_dim['FullHierarchyPathName'] = df_dim.apply(lambda row: calculate_path(row, 'Name'), axis=1)\n",
    "df_dim['FullHierarchyPathLabel'] = df_dim.apply(lambda row: calculate_path(row, 'Label'), axis=1)\n",
    "\n",
    "\n",
    "# Select and order final columns for DimType table\n",
    "final_dim_columns = [\n",
    "    'TypeSK', 'SourceTypeID',\n",
    "    'Level1_ID', 'Level1_Name', 'Level1_Label',\n",
    "    'Level2_ID', 'Level2_Name', 'Level2_Label',\n",
    "    'Level3_ID', 'Level3_Name', 'Level3_Label',\n",
    "    'Level4_ID', 'Level4_Name', 'Level4_Label',\n",
    "    'FullHierarchyPathName', 'FullHierarchyPathLabel', 'HierarchyDepth'\n",
    "]\n",
    "df_dim_type = df_dim[final_dim_columns].copy()\n",
    "\n",
    "# Convert Pandas <NA> or NaN to None for database compatibility\n",
    "df_dim_type = df_dim_type.astype(object).where(pd.notnull(df_dim_type), None)\n",
    "\n",
    "\n",
    "print(\"--- Final DimType DataFrame (with TypeSK) ---\")\n",
    "print(df_dim_type.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565a2445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "# --- Assume DimType DataFrame 'df_dim_type' is already created ---\n",
    "# Example df_dim_type (needs SourceTypeID, TypeSK, HierarchyDepth)\n",
    "# dim_data = {\n",
    "#     'TypeSK': [101, 102, 103, 104], # Example Surrogate Keys\n",
    "#     'SourceTypeID': [97, 75, 115, 116],\n",
    "#     'Level1_ID': [97, 97, 97, 97],\n",
    "#     'Level2_ID': [None, 75, 75, 75],\n",
    "#     'Level3_ID': [None, None, 115, 115],\n",
    "#     'Level4_ID': [97, 75, 115, 116],\n",
    "#     'HierarchyDepth': [1, 2, 3, 4] # Depths corresponding to the granular IDs\n",
    "#     # ... other DimType columns like names/labels\n",
    "# }\n",
    "# df_dim_type = pd.DataFrame(dim_data)\n",
    "df_dim_type = pd.DataFrame(df_dim)\n",
    "\n",
    "# --- Load Source Fact Data ---\n",
    "# fact_data = \"\"\"\n",
    "# \"id\",\"x\",\"y\",\"mp\",\"date\",\"state\",\"relevance\",\"types\"\n",
    "# 26994486,15.059215,50.770240,\"true\",\"2025-04-01T17:04:00.0000+02:00\",1,4,75\n",
    "# 26994486,15.059215,50.770240,\"true\",\"2025-04-01T17:04:00.0000+02:00\",1,4,97\n",
    "# 26994486,15.059215,50.770240,\"true\",\"2025-04-01T17:04:00.0000+02:00\",1,4,115\n",
    "# 26994486,15.059215,50.770240,\"true\",\"2025-04-01T17:04:00.0000+02:00\",1,4,116\n",
    "# 26994487,15.1,50.8,\"false\",\"2025-04-01T18:00:00.0000+02:00\",1,3,97\n",
    "# 26994488,15.2,50.9,\"true\",\"2025-04-01T19:00:00.0000+02:00\",2,5,75\n",
    "# 26994488,15.2,50.9,\"true\",\"2025-04-01T19:00:00.0000+02:00\",2,5,97\n",
    "# \"\"\"\n",
    "# df_source_facts = pd.read_csv(io.StringIO(fact_data))\n",
    "df_source_facts = pd.read_csv(\"C://Users//jirip//Documents//Developer//python//kriminalita//202504.csv\", sep=\",\", encoding=\"utf-8\")\n",
    "\n",
    "# --- Data Type Conversions (Important!) ---\n",
    "df_source_facts['date'] = pd.to_datetime(df_source_facts['date'])\n",
    "df_source_facts['mp'] = df_source_facts['mp'].map({'true': True, 'false': False})\n",
    "# Ensure 'types' column is numeric if it's not already\n",
    "df_source_facts['types'] = pd.to_numeric(df_source_facts['types'])\n",
    "\n",
    "\n",
    "# --- ETL Process to get one row per event with granular TypeSK ---\n",
    "\n",
    "# 1. Merge source facts with the relevant DimType info\n",
    "#    We need TypeSK and HierarchyDepth based on the 'types' column\n",
    "df_merged = pd.merge(\n",
    "    df_source_facts,\n",
    "    df_dim_type[['SourceTypeID', 'HierarchyDepth', 'TypeSK']],\n",
    "    left_on='types',\n",
    "    right_on='SourceTypeID',\n",
    "    how='inner'  # Assumes every 'types' value in facts exists in DimType\n",
    ")\n",
    "\n",
    "# 2. Identify the most granular row for each 'id'\n",
    "#    Sort by event id and then descending by depth, then keep the first row per id.\n",
    "df_merged_sorted = df_merged.sort_values(by=['id', 'HierarchyDepth'], ascending=[True, False])\n",
    "df_granular_facts = df_merged_sorted.drop_duplicates(subset=['id'], keep='first')\n",
    "\n",
    "# 3. Select columns for the final Fact Table structure\n",
    "#    We need the original fact columns (excluding 'types' and 'SourceTypeID')\n",
    "#    and the 'TypeSK' we got from the merge corresponding to the most granular type.\n",
    "final_fact_columns = [\n",
    "    'id', 'x', 'y', 'mp', 'date', 'state', 'relevance', # Original fact measures/degenerates\n",
    "    'TypeSK'  # Foreign key to DimType\n",
    "]\n",
    "df_final_fact_table = df_granular_facts[final_fact_columns]\n",
    "\n",
    "# --- Output ---\n",
    "print(\"--- Source Facts ---\")\n",
    "print(df_source_facts.to_string())\n",
    "print(\"\\n--- Dimension Type (Relevant Columns) ---\")\n",
    "print(df_dim_type[['TypeSK', 'SourceTypeID', 'HierarchyDepth']].to_string())\n",
    "print(\"\\n--- Merged Data (Facts + Dim Info) ---\")\n",
    "print(df_merged.to_string())\n",
    "print(\"\\n--- Identified Granular Rows (Staging) ---\")\n",
    "print(df_granular_facts.to_string())\n",
    "print(\"\\n--- Final Fact Table Data (One row per event) ---\")\n",
    "print(df_final_fact_table.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e939295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load your CSVs\n",
    "types_df = pd.read_csv(\"C://Users//jirip//Documents//Developer//python//kriminalita//types.csv\", sep=\",\", encoding=\"utf-8\")\n",
    "fact_df = pd.read_csv(\"C://Users//jirip//Documents//Developer//python//kriminalita//202504.csv\", sep=\",\", encoding=\"utf-8\")\n",
    "\n",
    "# Create a mapping of each type_id to its ancestors\n",
    "def get_ancestors(row):\n",
    "    return set([pid for pid in [row['parent_id1'], row['parent_id2'], row['parent_id3']] if pid > 0])\n",
    "\n",
    "type_ancestors = types_df.set_index('id').apply(get_ancestors, axis=1).to_dict()\n",
    "\n",
    "# Group fact table by `id` (your event) and process its type_ids\n",
    "def filter_most_granular(group):\n",
    "    type_ids = set(group['types'])\n",
    "    to_remove = set()\n",
    "\n",
    "    for t1 in type_ids:\n",
    "        for t2 in type_ids:\n",
    "            if t1 == t2:\n",
    "                continue\n",
    "            if t1 in type_ancestors.get(t2, set()):\n",
    "                to_remove.add(t1)\n",
    "\n",
    "    # Keep only the most granular types\n",
    "    filtered = type_ids - to_remove\n",
    "    return pd.DataFrame({'id': [group['id'].iloc[0]] * len(filtered), 'types': list(filtered)})\n",
    "\n",
    "# Apply filtering\n",
    "filtered_fact_df = fact_df.groupby('id', group_keys=False).apply(filter_most_granular).reset_index(drop=True)\n",
    "\n",
    "print(filtered_fact_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcc8ff32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your CSVs\n",
    "types_df = pd.read_csv(\"C://Users//jirip//Documents//Developer//python//kriminalita//types.csv\", sep=\",\", encoding=\"utf-8\")\n",
    "\n",
    "# Prepare lookup for type names/labels\n",
    "type_lookup = types_df.set_index('id')[['name', 'label']].to_dict('index')\n",
    "\n",
    "# Create expanded hierarchy table\n",
    "def expand_hierarchy(row):\n",
    "    ids = [row['parent_id1'], row['parent_id2'], row['parent_id3'], row['id']]\n",
    "    labels = [type_lookup.get(i, {'label': None})['label'] if i > 0 else None for i in ids]\n",
    "    names = [type_lookup.get(i, {'name': None})['name'] if i > 0 else None for i in ids]\n",
    "    \n",
    "    result = {}\n",
    "    for level, (tid, label, name) in enumerate(zip(ids, labels, names), 1):\n",
    "        result[f'level_{level}_id'] = tid if tid > 0 else None\n",
    "        result[f'level_{level}_label'] = label\n",
    "        result[f'level_{level}_name'] = name\n",
    "    return pd.Series(result)\n",
    "\n",
    "dimension_df = types_df.apply(expand_hierarchy, axis=1)\n",
    "dimension_df = pd.concat([types_df[['id']], dimension_df, types_df[['name', 'label']]], axis=1)\n",
    "\n",
    "# Save or inspect\n",
    "dimension_df.to_csv('dim_type.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "053eb131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing level_3 data from level_4\n",
    "mask = dimension_df['level_3_id'].isna()\n",
    "dimension_df.loc[mask, ['level_3_id', 'level_3_label', 'level_3_name']] = dimension_df.loc[mask, ['level_4_id', 'level_4_label', 'level_4_name']].values\n",
    "\n",
    "# Fill missing level_2 data from level_3\n",
    "mask = dimension_df['level_2_id'].isna()\n",
    "dimension_df.loc[mask, ['level_2_id', 'level_2_label', 'level_2_name']] = dimension_df.loc[mask, ['level_3_id', 'level_3_label', 'level_3_name']].values\n",
    "\n",
    "# Fill missing level_1 data from level_2\n",
    "mask = dimension_df['level_1_id'].isna()\n",
    "dimension_df.loc[mask, ['level_1_id', 'level_1_label', 'level_1_name']] = dimension_df.loc[mask, ['level_2_id', 'level_2_label', 'level_2_name']].values\n",
    "dimension_df.to_csv('dim_type.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83e331f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 310501 entries, 0 to 310500\n",
      "Data columns (total 8 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   id         310501 non-null  int64  \n",
      " 1   x          310501 non-null  float64\n",
      " 2   y          310501 non-null  float64\n",
      " 3   mp         310501 non-null  bool   \n",
      " 4   date       310501 non-null  object \n",
      " 5   state      310501 non-null  int64  \n",
      " 6   relevance  310501 non-null  int64  \n",
      " 7   types      310501 non-null  int64  \n",
      "dtypes: bool(1), float64(2), int64(4), object(1)\n",
      "memory usage: 16.9+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "fact_df = pd.read_csv(\"C://Users//jirip//Documents//Developer//python//kriminalita//202504.csv\", sep=\",\", encoding=\"utf-8\")\n",
    "fact_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77a948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your CSV\n",
    "types_df = pd.read_csv(\"C://Users//jirip//Documents//Developer//python//kriminalita//types.csv\", sep=\",\", encoding=\"utf-8\")\n",
    "\n",
    "# Prepare lookup for type names/labels\n",
    "type_lookup = types_df.set_index('id')[['name', 'label']].to_dict('index')\n",
    "\n",
    "# Expand hierarchy\n",
    "def expand_hierarchy(row):\n",
    "    ids = [row['parent_id1'], row['parent_id2'], row['parent_id3'], row['id']]\n",
    "    labels = [type_lookup.get(i, {'label': None})['label'] if i > 0 else None for i in ids]\n",
    "    names = [type_lookup.get(i, {'name': None})['name'] if i > 0 else None for i in ids]\n",
    "\n",
    "    # Shift hierarchy upwards if missing\n",
    "    combined = [\n",
    "        (tid if tid > 0 else None, label, name)\n",
    "        for tid, label, name in zip(ids, labels, names)\n",
    "    ]\n",
    "    # Remove empty levels\n",
    "    compacted = [(tid, label, name) for tid, label, name in combined if tid is not None]\n",
    "\n",
    "    # Fill up to 4 levels\n",
    "    while len(compacted) < 4:\n",
    "        compacted.append((None, None, None))\n",
    "\n",
    "    # Build the result\n",
    "    result = {}\n",
    "    for level, (tid, label, name) in enumerate(compacted, 1):\n",
    "        result[f'level_{level}_id'] = tid\n",
    "        result[f'level_{level}_label'] = label\n",
    "        result[f'level_{level}_name'] = name\n",
    "\n",
    "    return pd.Series(result)\n",
    "\n",
    "# Apply hierarchy expansion\n",
    "dimension_df = types_df.apply(expand_hierarchy, axis=1)\n",
    "\n",
    "# Combine with the original type id, name, and label\n",
    "dimension_df = pd.concat([types_df[['id']], dimension_df, types_df[['name', 'label']]], axis=1)\n",
    "\n",
    "# Save the result\n",
    "dimension_df.to_csv('dim_type.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ac1244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build full ancestor map recursively\n",
    "from collections import defaultdict\n",
    "\n",
    "parent_map = defaultdict(set)\n",
    "\n",
    "def collect_ancestors(type_id):\n",
    "    if type_id == 0 or pd.isna(type_id):\n",
    "        return set()\n",
    "    row = types_df[types_df['id'] == type_id]\n",
    "    if row.empty:\n",
    "        return set()\n",
    "    \n",
    "    direct_parents = set(row[['parent_id1', 'parent_id2', 'parent_id3']].values.flatten())\n",
    "    direct_parents = {pid for pid in direct_parents if pid > 0}\n",
    "    \n",
    "    ancestors = set(direct_parents)\n",
    "    for parent in direct_parents:\n",
    "        ancestors |= collect_ancestors(parent)\n",
    "    return ancestors\n",
    "\n",
    "# Build a cache\n",
    "type_ancestors = {tid: collect_ancestors(tid) for tid in types_df['id']}\n",
    "\n",
    "# Your fact table\n",
    "fact_df = pd.read_csv(\"C://Users//jirip//Documents//Developer//python//kriminalita//202504.csv\", sep=\",\", encoding=\"utf-8\")\n",
    "\n",
    "# Deduplicate for most granular types\n",
    "def filter_most_granular(group):\n",
    "    type_ids = set(group['types'])\n",
    "    to_remove = set()\n",
    "\n",
    "    for t1 in type_ids:\n",
    "        for t2 in type_ids:\n",
    "            if t1 == t2:\n",
    "                continue\n",
    "            if t1 in type_ancestors.get(t2, set()):\n",
    "                to_remove.add(t1)\n",
    "\n",
    "    filtered = type_ids - to_remove\n",
    "    return pd.DataFrame({'id': [group['id'].iloc[0]] * len(filtered), 'types': list(filtered), 'Longitude': group['x'].iloc[0], 'Latitude': group['y'].iloc[0], 'mp': group['mp'].iloc[0], 'date': group['date'].iloc[0], 'state': group['state'].iloc[0], 'relevance': group['relevance'].iloc[0]})\n",
    "\n",
    "filtered_fact_df = fact_df.groupby('id', group_keys=False).apply(filter_most_granular).reset_index(drop=True)\n",
    "\n",
    "# Save your cleaned fact table\n",
    "filtered_fact_df.to_csv('fact_clean.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a19cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign 'is_one' based on the order of 'types' within each 'id'\n",
    "filtered_fact_df['is_one'] = filtered_fact_df.sort_values(by=['id', 'types']).groupby('id').cumcount() + 1\n",
    "\n",
    "# Sort by 'id' and 'types' to ensure proper ordering\n",
    "filtered_fact_df = filtered_fact_df.sort_values(by=['id', 'types']).reset_index(drop=True)\n",
    "\n",
    "# Preview the updated DataFrame\n",
    "print(filtered_fact_df.head())\n",
    "\n",
    "# Save your cleaned fact table\n",
    "filtered_fact_df.to_csv('fact_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d442cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Define your file path\n",
    "file_path = \"C://Users//jirip//Documents//Developer//python//kriminalita//2023//2023_12_Prosinec_sest_01.xlsx\" \n",
    "match = re.search(r\"(\\d{4})_(\\d{2})\", file_path)\n",
    "year, month = match.groups() if match else (None, None)\n",
    "\n",
    "# Define your column headers manually\n",
    "columns = [\n",
    "    'TSK', 'TSK_desc', 'Registrovano_pocet', 'Objasneno_pocet', 'Objasneno_procent',\n",
    "    'Spachano_nezletilymi', 'Spachano_mladistvymi', 'Spachano_detmi', 'Spachano_opakovane', 'Spachano_cizinci',\n",
    "    'Spachano_pod_vlivem', 'Spachano_pod_vlivem_alkoholu', 'Spachano_firmou', 'Objasneno_dodatecne', 'Objasneno_celkem',\n",
    "    'Spachano_nezletilymi_celkem', 'Spachano_mladistvymi_celkem', 'Spachano_detmi_celkem', 'Spachano_opakovane_celkem', 'Spachano_cizinci_celkem',\n",
    "    'Spachano_pod_vlivem_celkem', 'Spachano_pod_vlivem_alkoholu_celkem', 'Spachano_firmou_celkem',\n",
    "     'Skoda_celkem_tis', 'Skoda_zajistena_tis'\n",
    "]  # Should match number of columns (Y-A + 1 = 25)\n",
    "\n",
    "# Load Excel file and get all sheet names\n",
    "xlsx = pd.ExcelFile(file_path, engine='openpyxl')\n",
    "\n",
    "# Container for all data\n",
    "all_data = []\n",
    "\n",
    "# Loop through each sheet\n",
    "for sheet_name in xlsx.sheet_names:\n",
    "    # Read the specific range (A7:Y304 -> rows 6 to 303 with zero indexing)\n",
    "    df = pd.read_excel(xlsx, sheet_name=sheet_name, usecols='A:Y', skiprows=6, nrows=275, header=None)\n",
    "    df.columns = columns\n",
    "    df['Region'] = sheet_name\n",
    "    df['Year'] = year\n",
    "    df['Month'] = month\n",
    "    all_data.append(df)\n",
    "# Concatenate into one DataFrame\n",
    "final_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Preview\n",
    "print(final_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d800055c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESS YEAR 2024\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "folder_path = 'C://Users//jirip//Documents//Developer//python//kriminalita//2024'  # Replace with your folder path\n",
    "columns = [\n",
    "    'TSK', 'TSK_desc', 'Registrovano_pocet', 'Objasneno_pocet', 'Objasneno_procent',\n",
    "    'Spachano_nezletilymi', 'Spachano_mladistvymi', 'Spachano_detmi', 'Spachano_opakovane', 'Spachano_cizinci',\n",
    "    'Spachano_pod_vlivem', 'Spachano_pod_vlivem_alkoholu', 'Spachano_firmou', 'Objasneno_dodatecne', 'Objasneno_celkem',\n",
    "    'Spachano_nezletilymi_celkem', 'Spachano_mladistvymi_celkem', 'Spachano_detmi_celkem', 'Spachano_opakovane_celkem', 'Spachano_cizinci_celkem',\n",
    "    'Spachano_pod_vlivem_celkem', 'Spachano_pod_vlivem_alkoholu_celkem', 'Spachano_firmou_celkem',\n",
    "     'Skoda_celkem_tis', 'Skoda_zajistena_tis'\n",
    "]\n",
    "\n",
    "# --- MAIN EXTRACTION ---\n",
    "all_data = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".xlsx\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Extract year and month from filename\n",
    "        match = re.search(r\"(\\d{4})_(\\d{2})\", filename)\n",
    "        if not match:\n",
    "            continue  # Skip files that don't match the pattern\n",
    "        year, month = map(int, match.groups())\n",
    "        period = datetime(year, month, 1)\n",
    "\n",
    "        # Load Excel file\n",
    "        xlsx = pd.ExcelFile(file_path, engine='openpyxl')\n",
    "\n",
    "        for sheet_name in xlsx.sheet_names:\n",
    "            df = pd.read_excel(\n",
    "                xlsx,\n",
    "                sheet_name=sheet_name,\n",
    "                usecols=\"A:Y\",\n",
    "                skiprows=6,\n",
    "                nrows=298,\n",
    "                header=None\n",
    "            )\n",
    "            df.columns = columns\n",
    "            df['Region'] = sheet_name\n",
    "            df['Period'] = period\n",
    "            all_data.append(df)\n",
    "\n",
    "# Combine all data into one DataFrame\n",
    "final_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Preview\n",
    "print(final_df.head())\n",
    "\n",
    "# Optional: Save output\n",
    "final_df.to_csv(\"combined_data_2024.csv\", index=False)\n",
    "# final_df.to_excel(\"combined_data.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d858ff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESS YEAR 2024\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "folder_path = 'C://Users//jirip//Documents//Developer//python//kriminalita//2024'  # Replace with your folder path\n",
    "columns = [\n",
    "    'TSK', 'TSK_desc', 'Registrovano_pocet', 'Objasneno_pocet', 'Objasneno_procent',\n",
    "    'Spachano_nezletilymi', 'Spachano_mladistvymi', 'Spachano_detmi', 'Spachano_opakovane', 'Spachano_cizinci',\n",
    "    'Spachano_pod_vlivem', 'Spachano_pod_vlivem_alkoholu', 'Spachano_firmou', 'Objasneno_dodatecne', 'Objasneno_celkem',\n",
    "    'Spachano_nezletilymi_celkem', 'Spachano_mladistvymi_celkem', 'Spachano_detmi_celkem', 'Spachano_opakovane_celkem', 'Spachano_cizinci_celkem',\n",
    "    'Spachano_pod_vlivem_celkem', 'Spachano_pod_vlivem_alkoholu_celkem', 'Spachano_firmou_celkem',\n",
    "     'Skoda_celkem_tis', 'Skoda_zajistena_tis'\n",
    "]\n",
    "\n",
    "# --- MAIN EXTRACTION ---\n",
    "all_data = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".xlsx\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Extract year and month from filename\n",
    "        match = re.search(r\"(\\d{4})_(\\d{2})\", filename)\n",
    "        if not match:\n",
    "            continue  # Skip files that don't match the pattern\n",
    "        year, month = map(int, match.groups())\n",
    "        period = datetime(year, month, 1)\n",
    "\n",
    "        # Load Excel file\n",
    "        xlsx = pd.ExcelFile(file_path, engine='openpyxl')\n",
    "\n",
    "        for sheet_name in xlsx.sheet_names:\n",
    "            df = pd.read_excel(\n",
    "                xlsx,\n",
    "                sheet_name=sheet_name,\n",
    "                usecols=\"A:Y\",\n",
    "                skiprows=306,\n",
    "                nrows=12,\n",
    "                header=None\n",
    "            )\n",
    "            df.columns = columns\n",
    "            df['Region'] = sheet_name\n",
    "            df['Period'] = period\n",
    "            all_data.append(df)\n",
    "\n",
    "# Combine all data into one DataFrame\n",
    "final_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Preview\n",
    "print(final_df.head())\n",
    "\n",
    "# Optional: Save output\n",
    "final_df.to_csv(\"AGG_data_2024.csv\", index=False)\n",
    "# final_df.to_excel(\"combined_data.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bd56ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESS YEAR 2023\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "folder_path = 'C://Users//jirip//Documents//Developer//python//kriminalita//2023'  # Replace with your folder path\n",
    "columns = [\n",
    "    'TSK', 'TSK_desc', 'Registrovano_pocet', 'Objasneno_pocet', 'Objasneno_procent',\n",
    "    'Spachano_nezletilymi', 'Spachano_mladistvymi', 'Spachano_detmi', 'Spachano_opakovane', 'Spachano_cizinci',\n",
    "    'Spachano_pod_vlivem', 'Spachano_pod_vlivem_alkoholu', 'Spachano_firmou', 'Objasneno_dodatecne', 'Objasneno_celkem',\n",
    "    'Spachano_nezletilymi_celkem', 'Spachano_mladistvymi_celkem', 'Spachano_detmi_celkem', 'Spachano_opakovane_celkem', 'Spachano_cizinci_celkem',\n",
    "    'Spachano_pod_vlivem_celkem', 'Spachano_pod_vlivem_alkoholu_celkem', 'Spachano_firmou_celkem',\n",
    "     'Skoda_celkem_tis', 'Skoda_zajistena_tis'\n",
    "]\n",
    "\n",
    "# --- MAIN EXTRACTION ---\n",
    "all_data = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".xlsx\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Extract year and month from filename\n",
    "        match = re.search(r\"(\\d{4})_(\\d{2})\", filename)\n",
    "        if not match:\n",
    "            continue  # Skip files that don't match the pattern\n",
    "        year, month = map(int, match.groups())\n",
    "        period = datetime(year, month, 1)\n",
    "\n",
    "        # Load Excel file\n",
    "        xlsx = pd.ExcelFile(file_path, engine='openpyxl')\n",
    "\n",
    "        for sheet_name in xlsx.sheet_names:\n",
    "            df = pd.read_excel(\n",
    "                xlsx,\n",
    "                sheet_name=sheet_name,\n",
    "                usecols=\"A:Y\",\n",
    "                skiprows=6,\n",
    "                nrows=275,\n",
    "                header=None\n",
    "            )\n",
    "            df.columns = columns\n",
    "            df['Region'] = sheet_name\n",
    "            df['Period'] = period\n",
    "            all_data.append(df)\n",
    "\n",
    "# Combine all data into one DataFrame\n",
    "final_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Preview\n",
    "print(final_df.head())\n",
    "\n",
    "# Optional: Save output\n",
    "final_df.to_csv(\"combined_data_2023.csv\", index=False)\n",
    "# final_df.to_excel(\"combined_data.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24bbb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESS YEAR 2023\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "folder_path = 'C://Users//jirip//Documents//Developer//python//kriminalita//2023'  # Replace with your folder path\n",
    "columns = [\n",
    "    'TSK', 'TSK_desc', 'Registrovano_pocet', 'Objasneno_pocet', 'Objasneno_procent',\n",
    "    'Spachano_nezletilymi', 'Spachano_mladistvymi', 'Spachano_detmi', 'Spachano_opakovane', 'Spachano_cizinci',\n",
    "    'Spachano_pod_vlivem', 'Spachano_pod_vlivem_alkoholu', 'Spachano_firmou', 'Objasneno_dodatecne', 'Objasneno_celkem',\n",
    "    'Spachano_nezletilymi_celkem', 'Spachano_mladistvymi_celkem', 'Spachano_detmi_celkem', 'Spachano_opakovane_celkem', 'Spachano_cizinci_celkem',\n",
    "    'Spachano_pod_vlivem_celkem', 'Spachano_pod_vlivem_alkoholu_celkem', 'Spachano_firmou_celkem',\n",
    "     'Skoda_celkem_tis', 'Skoda_zajistena_tis'\n",
    "]\n",
    "\n",
    "# --- MAIN EXTRACTION ---\n",
    "all_data = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".xlsx\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Extract year and month from filename\n",
    "        match = re.search(r\"(\\d{4})_(\\d{2})\", filename)\n",
    "        if not match:\n",
    "            continue  # Skip files that don't match the pattern\n",
    "        year, month = map(int, match.groups())\n",
    "        period = datetime(year, month, 1)\n",
    "\n",
    "        # Load Excel file\n",
    "        xlsx = pd.ExcelFile(file_path, engine='openpyxl')\n",
    "\n",
    "        for sheet_name in xlsx.sheet_names:\n",
    "            df = pd.read_excel(\n",
    "                xlsx,\n",
    "                sheet_name=sheet_name,\n",
    "                usecols=\"A:Y\",\n",
    "                skiprows=283,\n",
    "                nrows=12,\n",
    "                header=None\n",
    "            )\n",
    "            df.columns = columns\n",
    "            df['Region'] = sheet_name\n",
    "            df['Period'] = period\n",
    "            all_data.append(df)\n",
    "\n",
    "# Combine all data into one DataFrame\n",
    "final_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Preview\n",
    "print(final_df.head())\n",
    "\n",
    "# Optional: Save output\n",
    "final_df.to_csv(\"AGG_data_2023.csv\", index=False)\n",
    "# final_df.to_excel(\"combined_data.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee1af14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DE-YTD\n",
    "import pandas as pd\n",
    "\n",
    "# Load your CSV\n",
    "df = pd.read_csv(\"C://Users//jirip//Documents//Developer//python//kriminalita//combined_data_2024.csv\", parse_dates=[\"Period\"])\n",
    "\n",
    "# Define identifier columns and columns to transform\n",
    "id_cols = ['Region', 'TSK', 'TSK_desc', 'Period']\n",
    "value_cols = [col for col in df.columns if col not in id_cols]\n",
    "\n",
    "# Sort for diffing\n",
    "df = df.sort_values(by=['Region', 'TSK', 'Period'])\n",
    "\n",
    "# Apply .diff() to get monthly deltas\n",
    "df[value_cols] = df.groupby(['Region', 'TSK'])[value_cols].diff().fillna(df[value_cols])\n",
    "\n",
    "# Optional: round or convert types if needed\n",
    "df[value_cols] = df[value_cols].round(2)\n",
    "\n",
    "# Preview\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1199e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DE-YTD\n",
    "import pandas as pd\n",
    "df= pd.read_csv(\"C://Users//jirip//Documents//Developer//python//kriminalita//AGG_data_2024.csv\", parse_dates=[\"Period\"])\n",
    "\n",
    "# List of YTD columns to transform (excluding identifier and dimension columns)\n",
    "ytd_columns = [\n",
    "    col for col in df.columns\n",
    "    if col not in ['Region', 'TSK', 'TSK_desc', 'Period']\n",
    "]\n",
    "\n",
    "# Sort for correct chronological operations\n",
    "df = df.sort_values(by=['Region', 'TSK', 'Period'])\n",
    "\n",
    "# Group and transform YTD to monthly deltas\n",
    "def convert_ytd_to_monthly(group):\n",
    "    monthly = group[ytd_columns].diff().fillna(group[ytd_columns])\n",
    "    monthly.columns = [f\"{col}_monthly\" for col in monthly.columns]\n",
    "    return pd.concat([group[['Region', 'TSK', 'TSK_desc', 'Period']], monthly], axis=1)\n",
    "\n",
    "# Apply transformation\n",
    "df_monthly = df.groupby(['Region', 'TSK'], group_keys=False).apply(convert_ytd_to_monthly)\n",
    "\n",
    "# Preview\n",
    "print(df_monthly.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
