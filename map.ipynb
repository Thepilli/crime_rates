{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5afe985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POLIGONS for KRAJE, OKRESY, OBCE\n",
    "# -*- coding: utf-8 -*-\n",
    "import json\n",
    "from shapely.geometry import shape, Point\n",
    "\n",
    "\n",
    "# # Load your kraje polygons from JSON\n",
    "# with open('C://Users//jirip//Documents//Developer//python//kriminalita//map_files//kraje.json') as f:\n",
    "#     regions_data = json.load(f)\n",
    "# # Prepare polygons\n",
    "# regions = []\n",
    "# for feature in regions_data['features']:  # assuming GeoJSON-like structure\n",
    "#     region_id = feature['id']\n",
    "#     polygon = shape(feature['geometry'])  # shapely geometry from geojson\n",
    "#     regions.append((region_id, polygon))\n",
    "\n",
    "\n",
    "# Load your okresy polygons from JSON\n",
    "with open('C://Users//jirip//Documents//Developer//python//kriminalita//map_files//okresy.json') as f:\n",
    "    districts_data = json.load(f)\n",
    "# Prepare polygons\n",
    "districts = []\n",
    "for feature in districts_data['features']:  # assuming GeoJSON-like structure\n",
    "    district_id = feature['id']\n",
    "    polygon = shape(feature['geometry'])  # shapely geometry from geojson\n",
    "    districts.append((district_id, polygon))\n",
    "\n",
    "\n",
    "#  Load your obce polygons from JSON\n",
    "with open('C://Users//jirip//Documents//Developer//python//kriminalita//map_files//obce.json') as f:\n",
    "    cities_data = json.load(f)\n",
    "# Prepare polygons\n",
    "cities = []\n",
    "for feature in cities_data['features']:  # assuming GeoJSON-like structure\n",
    "    city_id = feature['id']\n",
    "    polygon = shape(feature['geometry'])  # shapely geometry from geojson\n",
    "    cities.append((city_id, polygon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c7976fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point is inside region: CZ0322000000\n"
     ]
    }
   ],
   "source": [
    "# POLIGONS for OBCE\n",
    "# -*- coding: utf-8 -*-\n",
    "import json\n",
    "from shapely.geometry import shape, Point\n",
    "# Load obce polygons from JSON\n",
    "with open('C://Users//jirip//Documents//Developer//python//kriminalita//map_files//okresy.json') as f:\n",
    "    regions_data = json.load(f)\n",
    "\n",
    "# Prepare polygons\n",
    "regions = []\n",
    "for feature in regions_data['features']:  # assuming GeoJSON-like structure\n",
    "    region_id = feature['id']\n",
    "    polygon = shape(feature['geometry'])  # shapely geometry from geojson\n",
    "    regions.append((region_id, polygon))\n",
    "\n",
    "# Example point you want to assign\n",
    "point = Point(13.295537,49.387969)  # (longitude, latitude)\n",
    "\n",
    "# Find which region contains the point\n",
    "assigned_region = None\n",
    "for region_id, polygon in regions:\n",
    "    if polygon.contains(point):\n",
    "        assigned_region = region_id\n",
    "        break\n",
    "\n",
    "if assigned_region:\n",
    "    print(f\"Point is inside region: {assigned_region}\")\n",
    "else:\n",
    "    print(\"Point is not inside any region.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c3f9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLATTENING GEOJSON\n",
    "# -*- coding: utf-8 -*-\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open('C://Users//jirip//Documents//Developer//python//kriminalita//202504.geojson', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "rows = []\n",
    "for feature in data['features']:\n",
    "    properties = feature['properties']\n",
    "    geometry = feature['geometry']\n",
    "    row = properties.copy()  # Start with properties\n",
    "    row['geometry_type'] = geometry['type']\n",
    "    row['longitude'] = geometry['coordinates']\n",
    "    row['latitude'] = geometry['coordinates'][1]\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f894b771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POLIGONS for OBCE & ASSIGNING POINTS TO POLYGONS\n",
    "# -*- coding: utf-8 -*-\n",
    "import json\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape, Point\n",
    "\n",
    "# Load obce polygons from JSON\n",
    "with open('C://Users//jirip//Documents//Developer//python//kriminalita//map_files//okresy.json') as f:\n",
    "    regions_data = json.load(f)\n",
    "\n",
    "# Prepare polygons\n",
    "regions = []\n",
    "for feature in regions_data['features']:  # assuming GeoJSON-like structure\n",
    "    region_id = feature['id']\n",
    "    polygon = shape(feature['geometry'])  # shapely geometry from geojson\n",
    "    regions.append((region_id, polygon))\n",
    "\n",
    "\n",
    "\n",
    "#  Load your obce polygons from JSON\n",
    "with open('C://Users//jirip//Documents//Developer//python//kriminalita//map_files//obce.json') as f:\n",
    "    cities_data = json.load(f)\n",
    "# Prepare polygons\n",
    "cities = []\n",
    "for feature in cities_data['features']:  # assuming GeoJSON-like structure\n",
    "    city_id = feature['id']\n",
    "    polygon = shape(feature['geometry'])  # shapely geometry from geojson\n",
    "    cities.append((city_id, polygon))\n",
    "\n",
    "\n",
    "\n",
    "# yearMonths = ['202501','202502','202503','202504','202401','202402','202403','202404','202405','202406','202407','202408','202409','202410','202411','202412','202301','202302','202303','202304','202305','202306','202307','202308','202309','202310','202311','202312','202201','202202','202203','202204','202205','202206','202207','202208','202209','202210','202211','202212','202101','202102','202103','202104','202105','202106','202107','202108','202109','202110','202111','202112','202001','202002','202003','202004','202005','202006','202007','202008','202009','202010','202011','202012','201901','201902','201903','201904','201905','201906','201907','201908','201909','201910','201911','201912','201801','201802','201803','201804','201805','201806','201807','201808','201809','201810','201811','201812','201701','201702','201703','201704','201705','201706','201707','201708','201709','201710','201711','201712','201601','201602','201603','201604','201605','201606','201607','201608','201609','201610','201611','201612','201501','201502','201503','201504','201505','201506','201507','201508','201509','201510','201511','201512','201401','201402','201403','201404','201405','201406','201407','201408','201409','201410','201411','201412','201301','201302','201303','201304','201305','201306','201307','201308','201309','201310','201311','201312','201201','201202','201203','201204','201205','201206','201207','201208','201209','201210','201211','201212']\n",
    "yearMonths = ['202501', '202502']\n",
    "\n",
    "for yearMonth in yearMonths:\n",
    "    df = pd.read_csv(\n",
    "        f\"C://Users//jirip//Documents//Developer//python//kriminalita//processed_files//fact_clean_{yearMonth}.csv\", sep=\",\", encoding=\"utf-8\")\n",
    "\n",
    "    # Prepare regions GeoDataFrame\n",
    "    gdf_regions = gpd.GeoDataFrame(\n",
    "        {'region_id': [r[0] for r in regions]}, geometry=[r[1] for r in regions])\n",
    "\n",
    "    # Prepare points GeoDataFrame\n",
    "    gdf_points = gpd.GeoDataFrame(\n",
    "        df, geometry=gpd.points_from_xy(df.Longitude, df.Latitude))\n",
    "\n",
    "    # Spatial join\n",
    "    gdf_joined = gpd.sjoin(gdf_points, gdf_regions,how='left', predicate='within')\n",
    "\n",
    "    # If needed, bring the result back to pandas\n",
    "    df_result = pd.DataFrame(gdf_joined.drop(columns=['geometry', 'index_right']))\n",
    "    # df_result['mp'] = df_result['mp'].astype(int)\n",
    "    # df_result['region_id'] = df_result['region_id'].str[:-6]\n",
    "\n",
    "    # Prepare cities GeoDataFrame\n",
    "    gdf_cities = gpd.GeoDataFrame({'city_id': [r[0] for r in cities]}, geometry=[r[1] for r in cities])\n",
    "\n",
    "    # Prepare points GeoDataFrame\n",
    "    gdf_points = gpd.GeoDataFrame(df_result, geometry=gpd.points_from_xy(df_result.Longitude, df_result.Latitude))\n",
    "\n",
    "    # Spatial join\n",
    "    gdf_joined = gpd.sjoin(gdf_points, gdf_cities,how='left', predicate='within')\n",
    "\n",
    "    # If needed, bring the result back to pandas\n",
    "    df_result_final = pd.DataFrame(gdf_joined.drop(columns=['geometry', 'index_right', 'Longitude', 'Latitude']))\n",
    "    df_result_final['mp'] = df_result_final['mp'].astype(int)\n",
    "    df_result_final['city_id'] = df_result_final['city_id'].fillna(df_result_final['region_id'])\n",
    "    df_result_final = df_result_final.drop(columns=['region_id'])\n",
    "    df_result_final.to_csv(f'F_Crime_{yearMonth}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e236e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE D_LOCATION CSV\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "# Load the JSON file into a DataFrame\n",
    "with open('C://Users//jirip//Documents//Developer//python//kriminalita//obce.json', 'r') as file:\n",
    "    obce_data = json.load(file)\n",
    "\n",
    "# Extract relevant data into a DataFrame\n",
    "obce_data = pd.json_normalize(obce_data['features'])\n",
    "obce_data = obce_data[['id', 'name']]\n",
    "print(obce_data.head())\n",
    "obce_data.to_csv('D_location.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e1c58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONWLOAD ALL DATA FROM URLS\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# dates = [202501,202502,202503,202504,'202401','202402','202403','202404','202405','202406','202407','202408','202409','202410','202411','202412','202301','202302','202303','202304','202305','202306','202307','202308','202309','202310','202311','202312','202201','202202','202203','202204','202205','202206','202207','202208','202209','202210','202211','202212','202101','202102','202103','202104','202105','202106','202107','202108','202109','202110','202111','202112','202001','202002','202003','202004','202005','202006','202007','202008','202009','202010','202011','202012','201901','201902','201903','201904','201905','201906','201907','201908','201909','201910','201911','201912','201801','201802','201803','201804','201805','201806','201807','201808','201809','201810','201811','201812','201701','201702','201703','201704','201705','201706','201707','201708','201709','201710','201711','201712','201601','201602','201603','201604','201605','201606','201607','201608','201609','201610','201611','201612','201501','201502','201503','201504','201505','201506','201507','201508','201509','201510','201511','201512','201401','201402','201403','201404','201405','201406','201407','201408','201409','201410','201411','201412','201301','201302','201303','201304','201305','201306','201307','201308','201309','201310','201311','201312','201201','201202','201203','201204','201205','201206','201207','201208','201209','201210','201211','201212']\n",
    " \n",
    "\n",
    "for date in dates:\n",
    "    # URL of the zip file\n",
    "    url = f'https://kriminalita.policie.gov.cz/api/v2/downloads/{date}.zip'\n",
    "\n",
    "    # Download the zip file\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        # Extract the zip file\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "            z.extractall('path_to_extract')  # Replace 'path_to_extract' with your desired directory\n",
    "        print(f\"Download and extraction complete for {date}.\")\n",
    "    else:\n",
    "        print(f\"Failed to download file for {date}. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16f73fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18433166\n"
     ]
    }
   ],
   "source": [
    "# COMBINE CSV FILES\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the folder containing the CSV files\n",
    "folder_path = 'mapped_files'  \n",
    "# List to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Iterate through all files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.csv'):  # Check if the file is a CSV\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        df = pd.read_csv(file_path)  # Read the CSV into a DataFrame\n",
    "        dataframes.append(df)  # Append the DataFrame to the list\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "print(len(combined_df))  # Display the first few rows of the combined DataFrame"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
