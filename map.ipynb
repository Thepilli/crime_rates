{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5afe985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from shapely.geometry import shape, Point\n",
    "\n",
    "\n",
    "# Load your kraje polygons from JSON\n",
    "with open('C://Users//jirip//Documents//Developer//python//kriminalita//kraje.json') as f:\n",
    "    regions_data = json.load(f)\n",
    "# Prepare polygons\n",
    "regions = []\n",
    "for feature in regions_data['features']:  # assuming GeoJSON-like structure\n",
    "    region_id = feature['id']\n",
    "    polygon = shape(feature['geometry'])  # shapely geometry from geojson\n",
    "    regions.append((region_id, polygon))\n",
    "\n",
    "\n",
    "# Load your okresy polygons from JSON\n",
    "with open('C://Users//jirip//Documents//Developer//python//kriminalita//okresy.json') as f:\n",
    "    districts_data = json.load(f)\n",
    "# Prepare polygons\n",
    "districts = []\n",
    "for feature in districts_data['features']:  # assuming GeoJSON-like structure\n",
    "    district_id = feature['id']\n",
    "    polygon = shape(feature['geometry'])  # shapely geometry from geojson\n",
    "    districts.append((district_id, polygon))\n",
    "\n",
    "\n",
    "#  Load your obce polygons from JSON\n",
    "with open('C://Users//jirip//Documents//Developer//python//kriminalita//obce.json') as f:\n",
    "    cities_data = json.load(f)\n",
    "# Prepare polygons\n",
    "cities = []\n",
    "for feature in cities_data['features']:  # assuming GeoJSON-like structure\n",
    "    city_id = feature['id']\n",
    "    polygon = shape(feature['geometry'])  # shapely geometry from geojson\n",
    "    cities.append((city_id, polygon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7976fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load your kraje polygons from JSON\n",
    "with open('C://Users//jirip//Documents//Developer//python//kriminalita//obce.json') as f:\n",
    "    regions_data = json.load(f)\n",
    "\n",
    "# Prepare polygons\n",
    "regions = []\n",
    "for feature in regions_data['features']:  # assuming GeoJSON-like structure\n",
    "    region_id = feature['id']\n",
    "    polygon = shape(feature['geometry'])  # shapely geometry from geojson\n",
    "    regions.append((region_id, polygon))\n",
    "\n",
    "# Example point you want to assign\n",
    "point = Point(14.414338,50.087987)  # (longitude, latitude)\n",
    "\n",
    "# Find which region contains the point\n",
    "assigned_region = None\n",
    "for region_id, polygon in regions:\n",
    "    if polygon.contains(point):\n",
    "        assigned_region = region_id\n",
    "        break\n",
    "\n",
    "if assigned_region:\n",
    "    print(f\"Point is inside region: {assigned_region}\")\n",
    "else:\n",
    "    print(\"Point is not inside any region.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c3f9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open('C://Users//jirip//Documents//Developer//python//kriminalita//202504.geojson', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "rows = []\n",
    "for feature in data['features']:\n",
    "    properties = feature['properties']\n",
    "    geometry = feature['geometry']\n",
    "    row = properties.copy()  # Start with properties\n",
    "    row['geometry_type'] = geometry['type']\n",
    "    row['longitude'] = geometry['coordinates']\n",
    "    row['latitude'] = geometry['coordinates'][1]\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a240a823",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C://Users//jirip//Documents//Developer//python//kriminalita//fact_clean.csv\", sep=\",\", encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f894b771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Prepare regions GeoDataFrame\n",
    "gdf_regions = gpd.GeoDataFrame({'region_id': [r[0] for r in regions]}, geometry=[r[1] for r in regions])\n",
    "\n",
    "# Prepare points GeoDataFrame\n",
    "gdf_points = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.Longitude, df.Latitude))\n",
    "\n",
    "# Spatial join\n",
    "gdf_joined = gpd.sjoin(gdf_points, gdf_regions, how='left', predicate='within')\n",
    "\n",
    "# If needed, bring the result back to pandas\n",
    "df_result = pd.DataFrame(gdf_joined.drop(columns=['geometry', 'index_right']))\n",
    "\n",
    "df_result.to_csv('F_Crime.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e236e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file into a DataFrame\n",
    "with open('C://Users//jirip//Documents//Developer//python//kriminalita//obce.json', 'r') as file:\n",
    "    obce_data = json.load(file)\n",
    "\n",
    "# Extract relevant data into a DataFrame\n",
    "obce_data = pd.json_normalize(obce_data['features'])\n",
    "obce_data = obce_data[['id', 'name']]\n",
    "print(obce_data.head())\n",
    "obce_data.to_csv('D_location.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e1c58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download and extraction complete for 202501.\n",
      "Download and extraction complete for 202502.\n",
      "Download and extraction complete for 202503.\n",
      "Download and extraction complete for 202504.\n"
     ]
    }
   ],
   "source": [
    "# DONWLOAD ALL DATA FROM URLS\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# dates = [202501,202502,202503,202504,'202401','202402','202403','202404','202405','202406','202407','202408','202409','202410','202411','202412','202301','202302','202303','202304','202305','202306','202307','202308','202309','202310','202311','202312','202201','202202','202203','202204','202205','202206','202207','202208','202209','202210','202211','202212','202101','202102','202103','202104','202105','202106','202107','202108','202109','202110','202111','202112','202001','202002','202003','202004','202005','202006','202007','202008','202009','202010','202011','202012','201901','201902','201903','201904','201905','201906','201907','201908','201909','201910','201911','201912','201801','201802','201803','201804','201805','201806','201807','201808','201809','201810','201811','201812','201701','201702','201703','201704','201705','201706','201707','201708','201709','201710','201711','201712','201601','201602','201603','201604','201605','201606','201607','201608','201609','201610','201611','201612','201501','201502','201503','201504','201505','201506','201507','201508','201509','201510','201511','201512','201401','201402','201403','201404','201405','201406','201407','201408','201409','201410','201411','201412','201301','201302','201303','201304','201305','201306','201307','201308','201309','201310','201311','201312','201201','201202','201203','201204','201205','201206','201207','201208','201209','201210','201211','201212']\n",
    " \n",
    "\n",
    "for date in dates:\n",
    "    # URL of the zip file\n",
    "    url = f'https://kriminalita.policie.gov.cz/api/v2/downloads/{date}.zip'\n",
    "\n",
    "    # Download the zip file\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        # Extract the zip file\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "            z.extractall('path_to_extract')  # Replace 'path_to_extract' with your desired directory\n",
    "        print(f\"Download and extraction complete for {date}.\")\n",
    "    else:\n",
    "        print(f\"Failed to download file for {date}. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84a721fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the folder containing the CSV files\n",
    "folder_path = 'map_files'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16f73fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id          x          y     mp                            date  state  \\\n",
      "0   739  17.316089  49.302210  False  2012-01-19T13:14:00.0000+01:00      1   \n",
      "1   739  17.316089  49.302210  False  2012-01-19T13:14:00.0000+01:00      1   \n",
      "2   739  17.316089  49.302210  False  2012-01-19T13:14:00.0000+01:00      1   \n",
      "3   739  17.316089  49.302210  False  2012-01-19T13:14:00.0000+01:00      1   \n",
      "4  1075  14.585108  50.226063  False  2012-01-26T16:00:00.0000+01:00      1   \n",
      "\n",
      "   relevance  types  \n",
      "0          1     35  \n",
      "1          1     55  \n",
      "2          1    101  \n",
      "3          1    102  \n",
      "4          4     74  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# List to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Iterate through all files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.csv'):  # Check if the file is a CSV\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        df = pd.read_csv(file_path)  # Read the CSV into a DataFrame\n",
    "        dataframes.append(df)  # Append the DataFrame to the list\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "print(combined_df.head())  # Display the first few rows of the combined DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b27e3728",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jirip\\AppData\\Local\\Temp\\ipykernel_21508\\2065586400.py:2: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  combined_df['date'] = pd.to_datetime(combined_df['date'])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can only use .dt accessor with datetimelike values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m combined_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(combined_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Extract the year from the 'date' column\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m combined_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mcombined_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[38;5;241m.\u001b[39myear\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Group by year and save each group as a separate CSV\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m year, group \u001b[38;5;129;01min\u001b[39;00m combined_df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\jirip\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py:6293\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   6287\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   6288\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   6289\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   6290\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   6291\u001b[0m ):\n\u001b[0;32m   6292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 6293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jirip\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\accessor.py:224\u001b[0m, in \u001b[0;36mCachedAccessor.__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessor\n\u001b[1;32m--> 224\u001b[0m accessor_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;66;03m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;66;03m# NDFrame\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, accessor_obj)\n",
      "File \u001b[1;32mc:\\Users\\jirip\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\accessors.py:643\u001b[0m, in \u001b[0;36mCombinedDatetimelikeProperties.__new__\u001b[1;34m(cls, data)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data\u001b[38;5;241m.\u001b[39mdtype, PeriodDtype):\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PeriodProperties(data, orig)\n\u001b[1;32m--> 643\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only use .dt accessor with datetimelike values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can only use .dt accessor with datetimelike values"
     ]
    }
   ],
   "source": [
    "# Ensure the 'date' column is in datetime format\n",
    "combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "\n",
    "# Extract the year from the 'date' column\n",
    "combined_df['year'] = combined_df['date'].dt.year\n",
    "\n",
    "# Group by year and save each group as a separate CSV\n",
    "for year, group in combined_df.groupby('year'):\n",
    "    group.to_csv(f'annual_data_{year}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
